<div id=toc></div>

# Table of Contents

- [cs.SC](#cs.SC) [Total: 2]
- [hep-ph](#hep-ph) [Total: 21]
- [hep-th](#hep-th) [Total: 10]
- [cs.AI](#cs.AI) [Total: 25]


<div id='cs.SC'></div>

# cs.SC [[Back]](#toc)

### [1] [A Variant of Non-uniform Cylindrical Algebraic Decomposition for Real Quantifier Elimination](https://arxiv.org/abs/2508.00505)
*Jasper Nalbach,Erika Ábrahám*

Main category: cs.SC

TL;DR: 该论文提出了改进的NuCAD算法，用于解决实代数问题，并在SMT求解和量词消去方面取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: CAD方法计算复杂度高，因此需要探索引导的适应性改进以避免部分计算，从而提升效率。

Method: 本文基于NuCAD算法设计了新的变种，同时支持量词消去和SMT求解，并进行了实验验证。

Result: 新NuCAD算法在实验中表现良好，能够与CAlC相媲美，且首次实现了完整的NuCAD实现。

Conclusion: 本文提出了NuCAD的新变种，并成功将其应用于真实量词消去和SMT求解，实验证明其效果与CAlC相当。

Abstract: The Cylindrical Algebraic Decomposition (CAD) method is currently the only
complete algorithm used in practice for solving real-algebraic problems. To
ameliorate its doubly-exponential complexity, different exploration-guided
adaptations try to avoid some of the computations. The first such adaptation
named NLSAT was followed by Non-uniform CAD (NuCAD) and the Cylindrical
Algebraic Covering (CAlC). Both NLSAT and CAlC have been developed and
implemented in SMT solvers for satisfiability checking, and CAlC was recently
also adapted for quantifier elimination. However, NuCAD was designed for
quantifier elimination only, and no complete implementation existed before this
work.
  In this paper, we present a novel variant of NuCAD for both real quantifier
elimination and SMT solving, provide an implementation, and evaluate the method
by experimentally comparing it to CAlC.

</details>


### [2] [Projective Delineability for Single Cell Construction](https://arxiv.org/abs/2508.00512)
*Jasper Nalbach,Lucas Michel,Erika Ábrahám,Christopher W. Brown,James H. Davenport,Matthew England,Pierre Mathonet,Naïm Zénaïdi*

Main category: cs.SC

TL;DR: 本文研究了基于投影可分解性的单细胞构造方法，以减少CAD算法在量词消去或SMT求解中的计算量。


<details>
  <summary>Details</summary>
Motivation: 尽管CAD算法具有双重指数复杂度，但其仍是解决量词消去或SMT求解问题的主要方法。近年来的探索引导算法如NLSAT、NuCAD和CAlC减少了计算工作量，因此本文尝试进一步改进单细胞构造的方法。

Method: 论文通过引入投影可分解性的弱概念，并调整单细胞构造方法，以减少计算量。

Result: 实验结果显示，利用投影可分解性的单细胞构造方法可以有效减少计算量。

Conclusion: 该论文得出结论，单细胞构造可以利用投影可分解性来减少计算量，并通过实验结果加以验证。

Abstract: The cylindrical algebraic decomposition (CAD) is the only complete method
used in practice for solving problems like quantifier elimination or SMT
solving related to real algebra, despite its doubly exponential complexity.
Recent exploration-guided algorithms like NLSAT, NuCAD, and CAlC rely on CAD
technology but reduce the computational effort heuristically. Single cell
construction is a paradigm that is used in each of these algorithms.
  The central property on which the CAD algorithm is based is called
delineability. Recently, we introduced a weaker notion called projective
delineability which can require fewer computations to guarantee, but needs to
be applied carefully. This paper adapts the single cell construction for
exploiting projective delineability and reports on experimental results.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [3] [Reconstructing Transition GPDs for Delta(1232) from Helicity Amplitude A_1/2(Q^2) via Dipole Fits and Impact Parameter Analysis](https://arxiv.org/abs/2508.00018)
*Ralph M. Marinaro III*

Main category: hep-ph

TL;DR: This paper reconstructs the GPD H_T(x,t) for the Delta(1232) resonance using data-driven methods, creating a model that links amplitude behavior to spatial structure with quantifiable results.


<details>
  <summary>Details</summary>
Motivation: The motivation is to better understand how longitudinal shaping modulates transverse localization in the Delta(1232) resonance by creating a physically interpretable map from amplitude behavior to spatial structure, using statistical diagnostics for quantification.

Method: The method involves a modular reconstruction based on digitized helicity amplitude data and dipole fits to A_1/2(Q^2), extracting a Sachs-like form factor F(t) to define a separable GPD model H_T(x, t) = h(x)F(t), where h(x) is modeled as a normalized Beta-like profile. This approach satisfies the GPD sum rule and allows for a two-dimensional Fourier transform to build transverse spatial distributions q(x,b).

Result: The result is a factorized ansatz for H_T(x,t) that satisfies the GPD sum rule and enables direct spatial distribution analysis through two-dimensional Fourier transforms, with quantified spatial features like mean radius, skewness, and kurtosis.

Conclusion: The paper concludes that their modular reconstruction of the GPD H_T(x,t) for the Delta(1232) resonance effectively translates amplitude behavior into spatial structure, offering a reproducible and data-driven framework applicable to other transition channels.

Abstract: We present a modular reconstruction of the transition generalized parton
distribution (GPD) H_T(x,t) for the Delta(1232) resonance, based on digitized
helicity amplitude data and dipole fits to A_1/2(Q^2). From the fitted
amplitude, we extract a Sachs-like form factor F(t) and define a separable GPD
model H_T(x, t) = h(x)F(t), with h(x) modeled as a normalized Beta-like
profile. This factorized ansatz satisfies the GPD sum rule and enables a direct
two-dimensional Fourier transform to construct transverse spatial distributions
q(x,b). We analyze how longitudinal shaping modulates transverse localization,
and quantify spatial features using statistical diagnostics including mean
radius, skewness, and kurtosis. The framework is reproducible, data-driven, and
applicable to other transition channels, providing a physically interpretable
map from amplitude behavior to spatial structure.

</details>


### [4] [Dark wounds on icy moons: Ganymede's subsurface ocean as a dark matter detector](https://arxiv.org/abs/2508.00054)
*William DeRocco*

Main category: hep-ph

TL;DR: 本文探讨了在木卫三上探测宏观复合暗物质碰撞的潜力，利用其分层结构和古老表面作为探测目标。


<details>
  <summary>Details</summary>
Motivation: 宏观复合形式的暗物质在特定质量范围内仍未被充分约束，可能与行星体碰撞并留下可观测的地质特征。

Method: 通过解析估算和iSALE多材料撞击代码的专用模拟，研究暗物质与木卫三的碰撞效应。

Result: 在参数空间的大部分区域，暗物质能够穿透木卫三的导电冰层，释放出次表层物质，这可能与表面冰层成分不同。

Conclusion: 即将前往木星系统的“欧罗巴黎普”和“果汁”任务不仅可能识别生命迹象，还可能识别暗物质撞击的迹象。

Abstract: Dark matter in the form of macroscopic composites is largely unconstrained at
masses of $\sim 10^{11}- 10^{17}$ g. In this mass range, dark matter may
collide with planetary bodies, depositing an immense amount of energy and
leaving dramatic surface features that remain detectable on geological
timescales. In this paper, we show that Ganymede, the largest Jovian moon,
provides a prime target to search for dark matter impacts due to its
differentiated composition and Gyr-old surface. We study the effects of dark
matter collisions with Ganymede first with analytic estimates, finding that in
a large region of parameter space, dark matter punches through Ganymede's
conductive ice sheet, liberating sub-surface material. This sub-surface
material may be compositionally different from the surface ice, providing a key
observable with which to discriminate asteroid impacts from those caused by
dark matter. We confirm our analytic estimates with dedicated simulations of
dark matter impacts using iSALE, a multi-material impact code. We then discuss
potential detection prospects with two missions currently en route to the
Jovian system, Europa Clipper and JUICE, finding that these missions may have
the ability not only to identify signs of life on the Galilean moons, but signs
of dark matter as well.

</details>


### [5] [Anomaly detection with spiking neural networks for LHC physics](https://arxiv.org/abs/2508.00063)
*Barry M. Dillon,Jim Harkin,Aqib Javed*

Main category: hep-ph

TL;DR: 这篇论文研究了使用 SNN AutoEncoders 进行 LHC 异常检测的可行性，并发现其性能与传统方法相当。


<details>
  <summary>Details</summary>
Motivation: 异常检测为发现新物理现象提供了有前途的策略，特别是在触发级别，它可以捕捉到传统选择切割会丢弃的信号。由于 SNN 固有的特性，它们特别适合在 FPGA 上进行低延迟、低内存和实时推理。

Method: 使用 CMS ADC2021 数据集设计并评估了一个简单的 SNN AutoEncoder 架构，并将其性能与传统 AutoEncoders 进行了比较。

Result: 结果表明，SNN AutoEncoders 在所有信号模型中都与传统 AutoEncoders 在 LHC 异常检测方面具有竞争力。

Conclusion: SNN AutoEncoders 是用于大型强子对撞机异常检测的有前途的工具，尤其适用于触发级别。

Abstract: Anomaly detection offers a promising strategy for discovering new physics at
the Large Hadron Collider (LHC). This paper investigates AutoEncoders built
using neuromorphic Spiking Neural Networks (SNNs) for this purpose. One key
application is at the trigger level, where anomaly detection tools could
capture signals that would otherwise be discarded by conventional selection
cuts. These systems must operate under strict latency and computational
constraints. SNNs are inherently well-suited for low-latency, low-memory,
real-time inference, particularly on Field-Programmable Gate Arrays (FPGAs).
Further gains are expected with the rapid progress in dedicated neuromorphic
hardware development. Using the CMS ADC2021 dataset, we design and evaluate a
simple SNN AutoEncoder architecture. Our results show that the SNN AutoEncoders
are competitive with conventional AutoEncoders for LHC anomaly detection across
all signal models.

</details>


### [6] [Worldline Modeling of Ultra-Intense Lasers for N-photon Scattering Processes](https://arxiv.org/abs/2508.00105)
*Ivan Ahumada,Patrick Copinger,James P. Edwards,Karthik Rajeev*

Main category: hep-ph

TL;DR: This paper presents a novel first-quantized path integral approach to model ultra-intense lasers and derive compact formulae for photon scattering in strong fields, avoiding perturbation theory.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to model present and future ultra-intense lasers, which require techniques beyond the standard diagrammatic approach to fully capture non-perturbative effects of strong fields.

Method: The method involves using a first-quantized path integral approach to strong-field quantum electrodynamics, enabling the treatment of lasers as background fields. An all-multiplicity construction for $N-$photon scattering processes is developed for complex scalars and spinors.

Result: Compact Master Formulae for tree-level scattering are derived, applicable to various background fields such as plane waves, impulsive PP-waves, non-null fields, and homogeneous fields with low-energy external photons.

Conclusion: The paper concludes that the first-quantized path integral representation provides a powerful tool for modeling ultra-intense lasers and capturing strong-field effects without perturbation theory.

Abstract: The modeling of present and future ultra-intense lasers demands techniques
that go beyond the standard diagrammatic approach to non-perturbatively fully
capture the effects of strong fields. We illustrate the first-quantized path
integral representation for strong-field quantum electrodynamics as a means of
accessing the laser being treated as a background field, which is treated
without recourse to perturbation theory. We examine an all-multiplicity
construction for $N-$photon scattering processes for complex scalars and
spinors, showing compact Master Formulae for tree-level scattering. Several
background fields are considering including: plane waves, impulsive PP-waves,
non-null fields, and homogeneous fields (constant-crossed fields) with
low-energy external photons.

</details>


### [7] [Distinguishing between Dirac and Majorana neutrinos at FASER](https://arxiv.org/abs/2508.00170)
*ShivaSankar K. A.,Alakabha Datta,Danny Marfatia*

Main category: hep-ph

TL;DR: 该研究探讨了FASER实验能否通过探测右手性中微子的衰变产物分布，区分其为狄拉克或马约拉纳粒子，并发现对于特定参数组合，实验可达到3σ的区分水平。


<details>
  <summary>Details</summary>
Motivation: 右手性中微子（RHNs）可以是狄拉克或马约拉纳粒子，这一区别对轻子数守恒和中微子的基本性质有深远影响。

Method: 利用蒙特卡罗模拟和χ²分析，研究右手性中微子在B、D、K和π介子衰变中的产生及其在FASER探测器中的三体衰变。

Result: 在右手性中微子静止参考系中，狄拉克和马约拉纳类型的衰变产物运动学和角度分布存在显著差异，这些差异表现为FASER实验中电子-正电子对的不同空间分布。

Conclusion: FASER实验有可能在3σ水平上区分右手性中微子的狄拉克或马约拉纳性质，通过分析电子-正电子对的空间分布。

Abstract: Some of the simplest models for the origin of neutrino mass involve
right-handed neutrinos (RHNs), which could be either Dirac or Majorana
particles - a distinction that has profound implications for lepton number
conservation and the fundamental nature of neutrinos. We investigate the
potential of the FASER experiment to distinguish between these two
possibilities using signatures predicted by the Standard Model Neutrino
Effective Field Theory (SMNEFT), where RHNs interact with Standard Model
particles through higher-dimensional operators. We focus on RHNs produced via
$B$, $D$, $K$, and $\pi$ meson decays at the Large Hadron Collider and their
subsequent three-body decays within the FASER detector. The kinematic and
angular distributions of the decay products in the RHN rest frame differ
significantly for Dirac and Majorana RHNs, and these differences manifest as
distinct spatial distributions of electron-positron pairs at FASER. Using Monte
Carlo simulations and a $\chi^2$ analysis, we demonstrate that these spatial
observables provide a robust experimental probe for determining the Dirac or
Majorana nature of RHNs. For select production and decay operator combinations
and RHN masses around 0.1 GeV, FASER can achieve discrimination at the
$3\sigma$ level.

</details>


### [8] [Variational Neural Network Approach to QFT in the Field Basis](https://arxiv.org/abs/2508.00173)
*Kevin Braga,Nobuo Sato,Adam P. Szczepaniak*

Main category: hep-ph

TL;DR: This paper introduces a neural network-based variational method for solving the free Klein-Gordon model in momentum space, demonstrating high accuracy and establishing a foundation for future applications to more complex models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of systematic benchmarking of neural network approaches for scalar field theories, particularly in the analytically solvable Klein-Gordon model in momentum space.

Method: The authors use a variational neural network approach to represent the ground-state wavefunctional of the Klein-Gordon model in momentum space. The neural network is trained by minimizing the Hamiltonian expectation value, allowing for direct comparison with exact analytic results.

Result: The framework successfully reproduces exact analytic results for key observables such as ground-state energy, two-point correlators, field expectation values, and the structure of the learned wavefunctional. Quantitative diagnostics confirm the method's accuracy.

Conclusion: The paper concludes that the neural network approach in momentum space is suitable for benchmarking and provides accurate results when compared to exact analytic solutions, laying the groundwork for future studies on interacting models and position-space formulations.

Abstract: We present a variational neural network approach for solving quantum field
theories in the field basis, focusing on the free Klein-Gordon model formulated
in momentum space. While recent studies have explored neural-network-based
variational methods for scalar field theory in position space, a systematic
benchmark of the analytically solvable Klein-Gordon ground state --
particularly in the momentum-space field basis -- has been lacking. In this
work, we represent the ground-state wavefunctional as a neural network defined
on a discretized set of field configurations and train it by minimizing the
Hamiltonian expectation value. This framework enables direct comparison to
exact analytic results for a range of key observables, including the
ground-state energy, two-point correlators, expectation value of the field, and
the structure of the learned wavefunctional itself. Our results provide
quantitative diagnostics of accuracy and demonstrate the suitability of
momentum space for benchmarking neural network approaches, while establishing a
foundation for future extensions to interacting models and position-space
formulations.

</details>


### [9] [Analytic Solution for the Helicity Evolution Equations at Small $x$ and Large $N_c\&N_f$](https://arxiv.org/abs/2508.00195)
*Jeremy Borden,Yuri V. Kovchegov*

Main category: hep-ph

TL;DR: The paper presents an exact solution for revised small-x helicity evolution equations, derives analytic expressions for helicity PDFs and g_1 structure function, and determines polarized DGLAP anomalous dimensions, showing consistency with existing calculations and slight disagreement with infrared evolution framework predictions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the behavior of helicity distributions at small-x by incorporating quark-to-gluon and gluon-to-quark transition operators in the evolution equations and to determine the intercept α_h for various N_c and N_f values.

Method: The method involves constructing an exact analytic solution of revised small-x helicity evolution equations in the large-N_c&N_f limit, deriving analytic expressions for helicity PDFs and the g_1 structure function, and extracting polarized DGLAP anomalous dimensions by resumming powers of α_s/ω².

Result: The results include analytic expressions for helicity PDFs and g_1 structure function as double-inverse Laplace transforms, resummed polarized DGLAP anomalous dimensions consistent with finite-order results, and numerical determination of the intercept α_h and the ratio of gluon to quark helicity PDFs at small-x.

Conclusion: The paper concludes that the predictions for polarized DGLAP anomalous dimensions align with existing finite-order calculations, and the intercept α_h shows minimal disagreement with infrared evolution equations framework predictions.

Abstract: We construct an exact analytic solution of the revised small-$x$ helicity
evolution equations, where the contributions of the quark-to-gluon and
gluon-to-quark transition operators were newly included. These evolution
equations are written in the large-$N_c\&N_f$ limit and are double-logarithmic,
resumming powers of $\alpha_s\ln^2(1/x)$. Here $N_c$ and $N_f$ are the numbers
of quark colors and flavors, while $\alpha_s$ is the strong coupling constant
and $x$ is the Bjorken-$x$ variable. Using our solution, we obtain analytic
expressions for the flavor singlet quark and gluon helicity parton distribution
functions (PDFs) and for the $g_1$ structure function as double-inverse Laplace
transforms. We also extract analytic expressions for the four DGLAP polarized
anomalous dimensions $\Delta \gamma_{qq}, \Delta \gamma_{qG}, \Delta
\gamma_{Gq}$, and $\Delta \gamma_{GG}$: these expressions resum powers of
$\alpha_s/\omega^2$ to all orders at large-$N_c\&N_f$ (with $\omega$ the Mellin
moment variable). We extract the leading small-$x$ growth of the helicity
distributions, \begin{align} \Delta\Sigma(x,Q^2) \sim \Delta G(x,Q^2)\sim
g_1(x,Q^2) \sim \left(\frac{1}{x}\right)^{\alpha_h}, \end{align} where the
intercept $\alpha_h$ satisfies an algebraic equation. We determine $\alpha_h$
numerically for various values of $N_c$ and $N_f$. We further obtain the
explicit asymptotic expressions for the helicity distributions, which yield
numerical values for the ratio of the gluon helicity PDF to the flavor singlet
quark helicity PDF in the small-$x$ asymptotic limit (for different $N_f/N_c$).
We find that all our predictions for polarized DGLAP anomalous dimensions are
fully consistent with the existing finite-order calculations. Similar to the
large-$N_c$ case, our intercept $\alpha_h$ exhibits a very slight disagreement
with the predictions made within the infrared evolution equations framework.

</details>


### [10] [Neutrino magnetic moment in the doublet-singlet Leptoquark model](https://arxiv.org/abs/2508.00226)
*Ricardo Sánchez-Vélez*

Main category: hep-ph

TL;DR: The paper investigates an extended Standard Model with Leptoquarks to generate a neutrino transition magnetic moment and explain B meson decay anomalies, finding consistency with experimental data.


<details>
  <summary>Details</summary>
Motivation: To explore the generation of neutrino transition magnetic moments and address anomalies in B meson decays using an extended Standard Model with Leptoquarks.

Method: The analysis involves studying the neutrino transition magnetic moment in an extended Standard Model with two scalar Leptoquarks, incorporating the latest measurements and experimental constraints.

Result: Large values for Leptoquark Yukawa couplings are found to be permissible, and the model aligns with experimental constraints and addresses the observed anomalies in B meson decays.

Conclusion: The study concludes that the Leptoquark model can generate a sizable neutrino transition magnetic moment and address anomalies in semileptonic B mesons decays while being consistent with recent experimental limits.

Abstract: The neutrino transition magnetic moment $\mu_{\nu_{\alpha \beta}}$ is studied
in a simple extension of the Standard Model. This extension incorporates two
scalar Leptoquarks $S_1$ and $\widetilde{R}_2$ with quantum numbers
$(\bar{3},1,1/3)$ and $(3,2,1/6)$ respectively. It is found that these
Leptoquarks generate a sizable transition magnetic moment, particularly when
the quark bottom is running in the loop. For our analysis of the parameter
space, we include the latest measurement of the muon magnetic moment and
combine it with the experimental constraint on the branching ratio Br$(\tau \to
\mu \gamma)$. We found that, despite the recent agreement on the $(g-2)_\mu$
value, large values for Leptoquark Yukawa couplings are allowed due to a
degeneracy in the parameters. Additionally, we explore how the Leptoquark model
address the anomalies observed in the ratios of semileptonic $B$ mesons decays,
$R_{D^{(*)}}$. We determine that the restrictions derived from our analysis are
consistent with the most recent experimental limits reported by the XENONnT and
LUX-ZEPLIN collaborations. This conclusion is based on our evaluation of the
transition magnetic moment from muon neutrino to tau neutrino, focusing on the
allowed region for the Leptoquark Yukawa couplings.

</details>


### [11] [Jet Image Generation in High Energy Physics Using Diffusion Models](https://arxiv.org/abs/2508.00250)
*Victor D. Martinez,Vidya Manian,Sudhir Malik*

Main category: hep-ph

TL;DR: The paper introduces diffusion models for generating jet images from LHC collision data, showing that consistency models outperform score-based models in accuracy and stability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the generation of jet images for proton-proton collision events at the LHC, providing more accurate and computationally efficient tools for High Energy Physics research.

Method: The authors used diffusion models, including score-based and consistency models, trained on jet images derived from kinematic variables of various particles to generate class-conditional jet images.

Result: Consistency models were found to achieve higher fidelity and generation stability compared to score-based diffusion models, as measured by metrics like the Fréchet Inception Distance (FID).

Conclusion: The paper concludes that consistency models outperform score-based diffusion models in generating high-fidelity jet images for High Energy Physics research.

Abstract: This article presents, for the first time, the application of diffusion
models for generating jet images corresponding to proton-proton collision
events at the Large Hadron Collider (LHC). The kinematic variables of quark,
gluon, W-boson, Z-boson, and top quark jets from the JetNet simulation dataset
are mapped to two-dimensional image representations. Diffusion models are
trained on these images to learn the spatial distribution of jet constituents.
We compare the performance of score-based diffusion models and consistency
models in accurately generating class-conditional jet images. Unlike approaches
based on latent distributions, our method operates directly in image space. The
fidelity of the generated images is evaluated using several metrics, including
the Fr\'echet Inception Distance (FID), which demonstrates that consistency
models achieve higher fidelity and generation stability compared to score-based
diffusion models. These advancements offer significant improvements in
computational efficiency and generation accuracy, providing valuable tools for
High Energy Physics (HEP) research.

</details>


### [12] [On the gravitational waves from super massive RHNs produced at preheating](https://arxiv.org/abs/2508.00315)
*Shinya Kanemura,Kunio Kaneta,Dibyendu Nanda*

Main category: hep-ph

TL;DR: The paper explores how supermassive particles produced after inflation can emit gravitational waves, offering a way to observe high-energy physics beyond the Standard Model through the resulting stochastic gravitational wave background.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the thermal history of the universe and explore the possibility of observing imprints of supermassive particles through their contribution to the gravitational wave background.

Method: The researchers analyzed the non-perturbative production of right-handed neutrinos in the context of α-attractor inflationary models, focusing on how these particles emit gravitons through bremsstrahlung upon decaying into Standard Model particles.

Result: The result is the identification of a stochastic gravitational wave background sourced by the decay of right-handed neutrinos, which carries indirect imprints of the heavy sector and post-inflationary dynamics.

Conclusion: The study concludes that the post-inflationary production of supermassive particles, particularly right-handed neutrinos, can lead to the emission of gravitons through bremsstrahlung, creating a stochastic gravitational wave background. This mechanism provides a way to observe imprints of high-scale physics beyond the Standard Model.

Abstract: The post-inflationary production of supermassive particles can have profound
implications for the thermal history of the universe and may leave observable
imprints in the gravitational wave (GW) background. In scenarios where the
inflaton couples predominantly to heavy fields, say right-handed neutrino
(RHN), non-perturbative mechanisms such as parametric resonance can lead to
their efficient production, even when their masses exceed the inflaton mass.
Once produced, the RHNs emit gravitons through bremsstrahlung as they decay
into the Standard Model (SM) particles via $N\rightarrow \ell + H$, enabled by
the unavoidable minimal coupling to gravity, sourcing a stochastic GW
background. We study this mechanism within the framework of $\alpha-$attractor
inflationary models, highlighting how the resulting GW spectrum carries
indirect imprints of the heavy sector and the post-inflationary dynamics. This
offers an observational window into otherwise inaccessible supermassive
particles and provides a powerful probe of high-scale physics beyond the SM.

</details>


### [13] [Analysis of the hadronic molecules $DK$, $D^*K$, $DK^*$ and their bottom analogs with QCD sum rules](https://arxiv.org/abs/2508.00402)
*Ze Zhou,Guo-Liang Yu,Zhi-Gang Wang,Jie Lu*

Main category: hep-ph

TL;DR: This paper uses QCD sum rules to predict tetraquark masses, showing that $BK^*$ might be a bound hadronic molecule.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the properties of tetraquark states and determine if certain molecular states are bound based on their mass predictions compared to their thresholds.

Method: Two-point QCD sum rules were used, considering vacuum condensates up to dimension 12, to calculate the masses and pole residues of charm-strange tetraquark states and their bottom analogs.

Result: The predicted masses for $DK$, $D^*K$, and $DK^*$ molecular states matched experimental data closely. Theoretical results for $BK$ and $B^*K$ were higher than their thresholds, while $BK^*$ was lower, suggesting it may be a bound state.

Conclusion: The study concludes that using color-singlet-color-singlet type currents with two-point QCD sum rules effectively predicts the masses and pole residues of tetraquark states, identifying potential bound states such as the $BK^*$ molecule.

Abstract: In this work, we construct the color-singlet-color-singlet type currents to
study the masses and pole residues of charm-strange tetraquark states and their
bottom analogs with $J^P$ = $0^+$ and $1^+$ by using two-point QCD sum rules,
where the vacuum condensates are considered up to dimension 12. The predicted
masses for $DK$, $D^*K$ and $DK^*$ molecular states are $2.322_{ - 0.072}^{ +
0.066}$ GeV, $2.457_{ - 0.068}^{ + 0.064}$ GeV and $2.538_{ - 0.062}^{ +
0.059}$ GeV. These results are consistent well with the experimental data of
$D_{s0}(2317)$, $D_{s1}(2460)$ and ${D}_{s1}(2536)$, respectively. The
theoretical results for $BK$ and $B^*K$ molecular states are $5.970_{ -
0.064}^{ + 0.061}$ GeV and $6.050_{ - 0.064}^{ + 0.062}$ GeV which are all
higher than their own thresholds. Finally, the mass of hadronic molecule $BK^*$
is predicted to be $6.158_{ - 0.063}^{ + 0.061}$ GeV. This value is lower than
the threshold of $BK^*$, which implies that it may be a bound hadronic
molecular state.

</details>


### [14] [Thermoelectric figure of merit and the deconfinement phase transition](https://arxiv.org/abs/2508.00407)
*Kamaljeet Singh,Raghunath Sahoo*

Main category: hep-ph

TL;DR: 这篇论文首次研究了高温QCD物质中热电优值（ZT）的行为，发现其在相变区域附近表现出非平凡的特性，有助于理解QCD物质在相变过程中的传输性质和自由度变化。


<details>
  <summary>Details</summary>
Motivation: 研究QCD物质在高温下的热电响应，可以深入了解其在相变区域中的微观动力学演化和有效自由度的再分配。

Method: 基于模型的计算方法，用于分析电导率、塞贝克系数和热导率的温度依赖性。

Result: 分析表明，ZT在QCD相变温度附近表现出特征性行为，反映了物质传输特性的变化。

Conclusion: 该论文首次对高温量子色动力学（QCD）物质中的热电优值（ZT）进行了现象学研究，表明其在相变区域附近表现出非平凡行为，反映了介质中传输特性和活性自由度的变化。

Abstract: Thermoelectric phenomena are traditionally associated with the
interconversion of thermal and electrical energy in many-body systems. In the
context of high-temperature quantum chromodynamics (QCD) matter produced in
relativistic heavy-ion collisions, thermoelectric responses can provide insight
into the evolving microscopic dynamics and the redistribution of effective
degrees of freedom across the phase transition region. In this work, for the
first time, we present a phenomenological study of the thermoelectric figure of
merit (\( ZT \)) in hot QCD matter, with a particular focus on its behavior
across the hadronic and quark-gluon plasma phases. Using model-based
calculations for the electrical conductivity, Seebeck coefficient, and thermal
conductivity, we analyze the temperature dependence of \( ZT \) and identify
characteristic features near the QCD phase transition temperature. Our results
indicate that \( ZT \) exhibits nontrivial behavior near the transition region,
reflecting the changing transport properties and active degrees of freedom in
the medium. This phenomenological study of the thermoelectric figure of merit
provides a complementary perspective to traditional transport studies and may
provide critical insights for advancing the understanding of QCD matter through
the transition region.

</details>


### [15] [On multi-propagator angular integrals](https://arxiv.org/abs/2508.00693)
*Juliane Haug,Vladimir A. Smirnov,Fabian Wunder*

Main category: hep-ph

TL;DR: The paper introduces a new method for simplifying and solving complex multi-propagator angular integrals using recursive reductions, differential equations, and dimensional recurrence, enabling higher precision calculations in particle physics.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop techniques for handling phase-space integrals by borrowing methods from loop-integral technology, specifically targeting multi-propagator angular integrals as a test case.

Method: The authors use an Euler integral representation similar to the Lee-Pomeransky approach, recursive IBP reduction, dimensional shift relations, and a differential equation approach to analyze multi-propagator angular integrals. They also employ dimensional recurrence and decomposition into branch integrals to simplify the problem.

Result: The authors explicitly compute previously unknown angular integrals with four denominators to finite order in ε, reduce the number of scales in master integrals using branch integrals, and derive all-order results in ε for massless three-denominator integrals, including soft logarithm resummation.

Conclusion: The paper concludes that the proposed method effectively reduces the complexity of multi-propagator angular integrals and enables all-order results in ε, showcasing its potential for simplifying phase-space integrals.

Abstract: We study multi-propagator angular integrals, a class of phase-space integrals
relevant to processes with multiple observed final states and a test-bed for
transferring loop-integral technology to phase space integrals without reversed
unitarity. We present an Euler integral representation similar to
Lee-Pomeransky representation and explicitly describe a recursive IBP reduction
and dimensional shift relations for the general case of $n$ denominators. On
the level of master integrals, applying a differential equation approach, we
explicitly calculate the previously unknown angular integrals with four
denominators for any number of masses to finite order in $\varepsilon$.
Extending the idea of dimensional recurrence, we explore the decomposition of
angular integrals into branch integrals reducing the number of scales in the
master integrals from $(n+1)n/2$ to $n+1$. To showcase the potential of this
method, we calculate the massless three denominator integral to establish
all-order results in $\varepsilon$ including a resummation of soft logarithms.

</details>


### [16] [Rare few-body decays of the Standard Model Higgs boson](https://arxiv.org/abs/2508.00466)
*David d'Enterria,Van Dung Le*

Main category: hep-ph

TL;DR: 本文综述了标准模型希格斯玻色子的罕见和排他性少体衰变，定义为那些分支比小于等于10^-5的衰变。这些衰变的研究可用于约束夸克和轻子的Yukawa耦合，探测味变希格斯衰变，估计超出标准模型粒子的奇异希格斯衰变背景，以及确认量子色动力学因子化。文章收集了约70个未观测到的希格斯玻色子衰变通道的理论分支比值，并估计了它们在HL-LHC上的预期界限。其中包括首次计算的20个新的衰变通道。


<details>
  <summary>Details</summary>
Motivation: 研究希格斯玻色子的罕见衰变有助于我们理解基本粒子间的相互作用，探索超出标准模型的新物理现象。

Method: 本文通过理论计算和实验数据收集，分析了约70个未观测到的希格斯玻色子衰变通道，包括20个首次计算的新通道。

Result: 文章提供了这些罕见衰变通道的理论分支比值，以及它们在HL-LHC上的预期实验限制。

Conclusion: 这项研究为未来实验和理论研究未观测到的希格斯玻色子衰变提供了指导和优先级。

Abstract: We present a survey of rare and exclusive few-body decays of the standard
model (SM) Higgs boson, defined as those into two to four final particles with
branching fractions $\mathcal{B}\lesssim 10^{-5}$. Studies of such decays can
be exploited to constrain Yukawa couplings of quarks and leptons, probe
flavour-changing Higgs decays, estimate backgrounds for exotic Higgs decays
into beyond-SM particles, and/or confirm quantum chromodynamics factorization
with small nonperturbative corrections. We collect the theoretical
$\mathcal{B}$ values for about 70 unobserved Higgs rare decay channels,
indicating their current experimental limits, and estimating their expected
bounds in p-p collisions at the HL-LHC. Among those, we include 20 new decay
channels computed for the first time for ultrarare Higgs boson decays into
photons and/or neutrinos, radiative quark-flavour-changing exclusive decays,
and radiative decays into leptonium states. This survey can help guide and
prioritize upcoming experimental and theoretical studies of unobserved Higgs
boson decays.

</details>


### [17] [Latin American network on electromagnetic effects in strongly interacting matter: Contribution to the update of the Latin American Strategy for High Energy, Cosmology and Astroparticle Physics](https://arxiv.org/abs/2508.00771)
*Ana Mizher,Alejandro Ayala*

Main category: hep-ph

TL;DR: The paper summarizes Latin American contributions to understanding electromagnetic effects in quark-gluon plasma and outlines a network's mission to boost global collaboration in this field.


<details>
  <summary>Details</summary>
Motivation: Understanding electromagnetic effects on strong force processes is crucial for characterizing quark-gluon plasma, which appears in extreme natural and laboratory conditions like the early universe and heavy-ion collisions.

Method: The paper outlines the contributions of Latin American researchers and describes the mission and future perspectives of the Latin American Network on Electromagnetic Effects in Strongly Interacting Matter.

Result: Recent contributions from Latin American researchers have advanced the study of electromagnetic effects in strongly interacting matter, with plans to strengthen collaborations and leverage upcoming experimental facilities.

Conclusion: The Latin American Network aims to enhance collaboration and interaction among researchers to explore the properties of strongly interacting matter in intense electromagnetic fields, connecting Latin American institutions globally.

Abstract: An accurate characterization of the quark-gluon plasma requires understanding
of how electromagnetic effects affect the processes mediated by the strong
force. All the scenarios in which the plasma emerges, either in nature or in
the laboratory, involve strong electromagnetic fields. The early universe,
compact astrophysical objects, or ultra-relativistic heavy-ion collisions
harbor the most intense fields we know. Researches from the Latin America
region have made a substantial contribution on this subject and the \lq\lq
Latin American Network on Electromagnetic Effects in Strongly Interacting
Matter" aims to cluster efforts to address open questions related to these
systems, boosting collaborations and interaction among its members and
connecting Latin American institutions with institutions from the rest of the
world. In face of the upcoming experimental programs and new facilities, our
mission is to bring together experimentalists, phenomenologists and theorists
to better explore the properties of strongly interacting matter in the presence
of intense electromagnetic fields. This document describes succinctly the
recent contributions from researchers of the Latin American region to the
subject, as well as our activities and perspectives for the future.

</details>


### [18] [Improved Limits on Exotic Interactions Mediated by Axion-Like Particles Between Muons](https://arxiv.org/abs/2508.00504)
*L. Y. Wu,H. Yan*

Main category: hep-ph

TL;DR: The paper updates constraints on exotic muon interactions using the latest muon anomalous magnetic moment measurements.


<details>
  <summary>Details</summary>
Motivation: The motivation is to constrain exotic interactions between muons mediated by new scalar or vector particles through the precise measurement of the muon AMDM.

Method: The method involves analyzing the precise measurement of the muon anomalous magnetic dipole moment (AMDM) and comparing experimental results with theoretical predictions.

Result: The result is the derivation of updated constraints on exotic interactions between muons, with the latest result for the muon AMDM being Δaμ = (38 ± 63) × 10⁻¹¹.

Conclusion: The study concludes that the updated constraints on exotic interactions between muons have been derived based on the latest results for the muon AMDM.

Abstract: The precise measurement of the muon anomalous magnetic dipole moment (AMDM)
$a_\mu$ provides an opportunity for constraining the exotic interactions
between muons mediated by new scalar or vector particles. Recent progress in
both experimental measurements and theoretical predictions of the muon AMDM has
reconciled the long-standing tension between them. Based on the latest result
for the muon AMDM, $\Delta a_\mu =a^{\rm exp}_\mu-a^{\rm SM}_\mu= (38 \pm 63)
\times 10^{-11}$, we derive updated constraints on exotic interactions between
muons.

</details>


### [19] [The SN 1987A Cooling Bound on Dark Matter Absorption in Electron Targets](https://arxiv.org/abs/2508.00725)
*Claudio Andrea Manzari,Jorge Martin Camalich,Jonas Spinner,Robert Ziegler*

Main category: hep-ph

TL;DR: This paper analyzes dark matter interactions using supernova cooling data, showing that current and upcoming direct detection experiments' sensitivity regions are excluded by these bounds, and extends the analysis to include light mediators, finding similar exclusion results when combining supernova and overproduction constraints.


<details>
  <summary>Details</summary>
Motivation: The motivation was to explore the parameter space relevant for direct detection experiments involving sub-MeV fermionic dark matter and to determine the validity of previous conclusions when considering light mediators.

Method: The researchers extended their analysis on sub-MeV fermionic dark matter to sub-GeV mediators, examining their resonant production in supernova cores and in the early Universe, and assessed the implications for direct detection experiments.

Result: The study found that the previously derived bounds on sub-MeV fermionic dark matter excluded the projected sensitivity regions of current and upcoming direct detection experiments. However, these conclusions did not hold for light mediators, prompting an extended analysis.

Conclusion: The study concludes that a combination of supernova cooling constraints and limits from dark matter overproduction excludes the entire parameter space relevant for direct detection in the case of sub-GeV mediators.

Abstract: We present new supernova (SN 1987A) cooling bounds on sub-MeV fermionic dark
matter with effective couplings to electrons. These bounds probe the parameter
space relevant for direct detection experiments in which dark matter can be
absorbed by the target material, showing strong complementarity with indirect
searches and constraints from dark matter overproduction. Crucially, our limits
exclude the projected sensitivity regions of current and upcoming direct
detection experiments. Since these conclusions are a priori not valid for light
mediators, we extend our analysis to this case. We show that sub-GeV mediators
can be produced resonantly both in supernova cores and in the early Universe,
altering the SN 1987A analysis for effective couplings. Still, a combination of
supernova cooling constraints and limits from dark matter overproduction
excludes the entire parameter space relevant for direct detection in this case.

</details>


### [20] [Non-Standard Neutrino Interactions at a Muon Collider Neutrino Detector](https://arxiv.org/abs/2508.00761)
*Felix Kling,Yang Ma,Krzysztof Mękała,Jürgen Reuter,Zahra Tabrizi*

Main category: hep-ph

TL;DR: 本文探讨了未来多TeV级缪子对撞机作为高强度中微子源在探索非标准中微子相互作用方面的巨大潜力，并提出了相应的前向中微子探测器的设计要求。


<details>
  <summary>Details</summary>
Motivation: 未来多TeV级缪子对撞机不仅具有高能量和精确度，还可能成为世界上最强烈的中微子源，从而可以探索新的非标准中微子相互作用。

Method: 通过在对撞环的直线部分安装专用的前向中微子探测器，测量来自缪子衰变的中微子引发的反应。

Result: 这种探索非标准中微子相互作用的能力可以超过当前和即将进行的低能量精密实验以及LHC的限制。

Conclusion: 未来多TeV级缪子对撞机可以作为世界上最强烈的中微子源，为探索非标准中微子相互作用提供独特机会。

Abstract: In addition to their broad physics reach enabled by their high energies and
precision, future multi-TeV muon colliders will also be the world's most
intense sources of neutrinos. This offers the opportunity to search for new
non-standard neutrino interactions, possible by installing a dedicated forward
neutrino detector in the straight sections of the collision ring, which is then
used to measure reactions initiated by neutrinos from the decaying beam muons.
In this paper, we show that these searches can exceed current and upcoming
bounds on non-standard neutrino interactions from low-energy precision
experiments and the LHC. This is achieved by the large flux of high-energetic
neutrinos, the precise knowledge of the neutrino flavor composition on each
side of the interaction point and the chirality of the neutrinos. We further
discuss the technical requirements of the proposed forward neutrino detector,
\FASERmuC, to maximally exploit this physics potential.

</details>


### [21] [Searching for charged Higgs bosons with flavor-changing couplings at the LHC](https://arxiv.org/abs/2508.00764)
*Mohamed Krab*

Main category: hep-ph

TL;DR: This paper explores the LHC's potential to discover charged Higgs bosons in a model with FCNH couplings, which could help explain the matter-antimatter asymmetry in the Universe.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore the discovery potential of charged Higgs bosons at the LHC, which could signal new physics described by the G2HDM with FCNH couplings and possibly explain the disappearance of antimatter.

Method: The researchers analyzed the resonant production of charged Higgs bosons through the process $ c\bar{b} \to H^+ $, followed by the decay $ H^+ \to W^+ H $, in the context of the G2HDM with FCNH couplings.

Result: The research shows that sizable $ \rho_{tc} $ couplings can significantly enhance charged Higgs boson production rates, making it a promising avenue for discovering G2HDM with FCNH couplings.

Conclusion: The study concludes that investigating charged Higgs boson production at the LHC can provide evidence for the G2HDM with FCNH couplings and potentially explain the baryon asymmetry of the Universe.

Abstract: We investigate the LHC discovery prospects for charged Higgs bosons in the
general two Higgs doublet model (G2HDM) that has flavor-changing neutral Higgs
(FCNH) couplings. The FCNH $\rho_{tc}$ coupling induces intriguing $H^+$
production processes $c\bar b \to H^+$, $cg \to bH^+$, and $\bar bg \to \bar
cH^+$ without CKM suppression. Sizable $\rho_{tc}$ can drive the disappearance
of antimatter from the Universe. In this contribution, we promote the resonant
$c\bar b \to H^+$ production, followed by the bosonic $H^+ \to W^+ H$ decay.
Discovery could be a harbinger of G2HDM with FCNH couplings, and perhaps shed
light on the baryon asymmetry of the Universe.

</details>


### [22] [$ψ(2S)$ production in jets using NRQCD](https://arxiv.org/abs/2508.00814)
*Marston Copeland,Lin Dai,Yu Fu,Jyotirmoy Roy*

Main category: hep-ph

TL;DR: 本研究利用LHCb数据和NRQCD框架下的FJF与GFIP方法，改进了ψ(2S)粒子在喷注中产生过程的理论预测，并指出当前LDME提取方法存在较大差异，需要更精确的提取。


<details>
  <summary>Details</summary>
Motivation: 基于LHCb的最新数据，旨在改进ψ(2S)粒子在喷注中产生机制的理论预测，并测试不同LDME（长程矩阵元）提取方法的差异。

Method: 使用非相对论QCD（NRQCD）结合Fragmenting Jet Function（FJF）和Gluon Fragmentation Improved Pythia（GFIP）方法，对ψ(2S)粒子在喷注中的产生进行了研究。

Result: FJF和GFIP方法比默认的Pythia+NRQCD更好地描述了实验数据；ψ(2S)分布能够有效区分不同LDME预测；不同合作组的预测结果差异显著。

Conclusion: 研究发现FJF和GFIP形式比默认的Pythia+NRQCD预测更能准确描述实验数据，ψ(2S)在喷注中的分布是测试不同LDME预测的优秀判据，不同合作组的预测差异较大，可能需要更精确地提取ψ(2S) LDMEs。

Abstract: Based on recent data from LHCb, we study $\psi(2S)$ production in jets using
non-relativistic QCD (NRQCD) in conjunction with the Fragmenting Jet Function
(FJF) and Gluon Fragmentation Improved Pythia (GFIP) formalisms. Similar to
previous studies of $J/\psi$ production in jets, our results show that these
formalisms offer a much better description of data than the default
Pythia+NRQCD prediction. We compare and contrast between the predictions from
the FJF formalism and the GFIP approach. In addition, our results show that the
distribution of $\psi(2S)$ in jets is an excellent discriminator to test
different predictions for the $\psi(2S)$ LDMEs from various extractions. We
find a large disparity between the predictions from three different
collaborations showing that a more precise extraction of the $\psi(2S)$ LDMEs
may be necessary.

</details>


### [23] [String-based axial and helicity-flip GPDs: a comparison to lattice QCD](https://arxiv.org/abs/2508.00817)
*Florian Hechenberger,Kiminad A. Mamo,Ismail Zahed*

Main category: hep-ph

TL;DR: 本研究提出了一种新的解析方法，利用字符串理论描述核子的广义部分子分布特性，并通过实验数据和格点QCD验证其预测能力。


<details>
  <summary>Details</summary>
Motivation: 为了构建一种适用于任意偏斜度的核子轴向和螺旋翻转广义部分子分布的解析字符串表示方法，同时满足多项式性、交叉对称性和支持性。

Method: 利用Mellin Barnes对共形部分波展开式进行求和，通过开放和闭合字符串轨迹参数化矩，并应用NLO DGLAP ERBL演化进行验证。

Result: 该解析框架能够重现部分现有的格点矩结果，预测了可通过未来实验验证的海夸克和胶子极化矩，并在x空间中与格点QCD结果基本一致。

Conclusion: 研究得出了一种基于字符串表示的解析方法，可用于描述核子的轴向和螺旋翻转共形矩，且该方法在多个实验中表现出良好的预测能力。

Abstract: We construct an analytic, string based representation of the nucleon's axial
and helicity flip conformal moments of generalized parton distributions that
holds for any skewness and for both the quark and gluon channels. The starting
point is the Mellin Barnes resummation of the conformal partial wave expansion,
where the moments are parametrized by open (Reggeon) and closed string
(Pomeron) trajectories with slopes determined by experimental form factors and
meson/glueball spectroscopy. The forward limits are fixed by the empirical
unpolarized and polarized parton distributions. Polynomiality, crossing
symmetry and support are satisfied by construction. After NLO DGLAP ERBL
evolution to $\mu=2$ GeV our analytic framework (i) reproduces some of the
currently available lattice moments of $\mathbb{E}$ and
$\widetilde{\mathbb{H}}$ in the non singlet sector, (ii) predicts sea quark and
gluon polarized moments that will be testable by forthcoming simulations and
experiments at Jefferson Lab and the future EIC, and (iii) yields axial and
helicity flip GPDs in $x$ space in reasonable agreement with lattice QCD.

</details>


<div id='hep-th'></div>

# hep-th [[Back]](#toc)

### [24] [Total instanton restriction via multiverse interference: Noncompact gauge theories and (-1)-form symmetries](https://arxiv.org/abs/2508.00050)
*Alonso Perez-Lona,Eric Sharpe,Xingyang Yu,Hao Zhang*

Main category: hep-th

TL;DR: 这篇论文研究了量子场论中分解方法的应用，通过拓扑规范化来消除瞬子，并在多个量子场论模型中进行了应用研究。


<details>
  <summary>Details</summary>
Motivation: 作者旨在解决局部量子场论中的瞬子问题，并探索分解方法在不同量子场论模型中的应用。

Method: 作者使用了连续族宇宙的分解方法，并通过拓扑规范化的-1形式对称性来消除局部量子场论中的所有瞬子。

Result: 成功地提出了一种新的方法来消除局部量子场论中的所有瞬子，并在二维U(1)规范理论中等效于将规范群改为R。此外，作者还澄清了Gross-Taylor字符串对二维纯杨-米尔斯分解的解释，并研究了具有R因子的二维规范理论和超对称规范线性σ模型。

Conclusion: 作者提出了一种新的方法，通过拓扑规范化的-1形式对称性来消除局部量子场论中的所有瞬子，并应用这种分解来澄清Gross-Taylor字符串对二维纯杨-米尔斯分解的解释，以及研究具有R因子的二维规范理论和超对称规范线性σ模型。

Abstract: In this note we consider examples of decomposition (in which a local QFT is
equivalent to a disjoint union of multiple independent theories, known as
universes) where there is a continuous familiy of universes, rather than a
finite or countably infinite collection. In particular, this allows us to
consistently eliminate all instantons in a local QFT via a suitable topological
gauging of the (-1)-form symmetry. In two-dimensional U(1) gauge theories, this
is equivalent to changing the gauge gruop to R. This makes both locality as
well as the instanton restriction explicit. We apply this to clarify the
Gross-Taylor string interpretation of the decomposition of two-dimensional pure
Yang-Mills. We also apply decomposition to study two-dimensional R gauge
theories, such as the pure R Maxwell theory, and two-dimensional supersymmetric
gauged linear sigma models whose gauge groups have factors of R. In that
context, we find that analogues of the Witten effect for dyons, here rotating
between universes, play a role in relating anomalies of the individual
universes to (different) anomalies in the disjoint union. Finally, we discuss
limits of the Tanizaki-Unsal construction, which accomplish instanton
restriction by topologically gauging a Q/Z (-1)-form symmetry, and speculate in
two-dimensional theories on possible interpretations of those limits in terms
of the adelic solenoid.

</details>


### [25] [Volume as an index of a subalgebra](https://arxiv.org/abs/2508.00056)
*Samuel Leutheusser,Hong Liu*

Main category: hep-th

TL;DR: The paper proposes a 'volume-index' relation in AdS spacetime, linking bulk volume to an algebraic quantity called the index of inclusion, providing a new boundary explanation for black hole interior volume growth and complexity growth from the Heisenberg picture.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the volume of subregions in the bulk of AdS spacetime and to provide a boundary explanation for phenomena such as black hole interior volume growth.

Method: The authors use the concept of subregion-subalgebra duality and the index of inclusion to relate the volume of bulk subregions in AdS spacetime to algebraic quantities on the boundary.

Result: The result is the proposal of a 'volume-index' relation that equates the exponential of the volume of a maximal volume slice with the index of inclusion, offering insights into complexity growth and other applications.

Conclusion: The paper concludes that the exponential of the volume of a maximal volume slice of a bulk subregion equals the index of inclusion, which provides a new boundary explanation for black hole interior volume growth.

Abstract: We propose a new way to understand the volume of certain subregions in the
bulk of AdS spacetime by relating it to an algebraic quantity known as the
index of inclusion. This index heuristically measures the relative size of a
subalgebra $\mathcal{N}$ embedded within a larger algebra $\mathcal{M}$.
According to subregion-subalgebra duality, bulk subregions are described by von
Neumann algebras on the boundary. When a causally complete bulk subregion
corresponds to the relative commutant $\mathcal{N}' \cap \mathcal{M}$ -- the
set of operators in $\mathcal{M}$ that commute with $\mathcal{N}$ -- of
boundary subalgebras, we propose that the exponential of the volume of the
maximal volume slice of the subregion equals the index of inclusion. This
``volume-index'' relation provides a new boundary explanation for the growth of
interior volume in black holes, reframing it as a change in the relative size
of operator algebras. It offers a complementary perspective on complexity
growth from the Heisenberg picture, and has a variety of other applications,
including quantifying the relative size of algebras dual to the entanglement
wedge and the causal wedge of a boundary region, as well as quantifying the
violation of additivity of operator algebras in the large $N$ limit. Finally,
it may offer insights into the volume growth of de Sitter space through the
changes in North and South pole observer algebras in time.

</details>


### [26] [Entanglement spreading and emergent locality in Brownian SYK chains](https://arxiv.org/abs/2508.00060)
*Onkar Parrikar,Jatin Narde,Harshit Rajgadia,Sandip Trivedi*

Main category: hep-th

TL;DR: This paper demonstrates that the spread of quantum information in a chaotic system exhibits a sharp light-cone behavior governed by the FKPP equation, providing a physical explanation for emergent locality and operator growth dynamics.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the Ryu-Takayanagi formula and quantum error correction interpretation, aiming to understand emergent locality in holographic CFTs and its connection to quantum information spread in chaotic systems.

Method: Using quantum error correction tools, the authors analyzed quantum information spread in a one-dimensional Brownian SYK chain, calculating the correlation of a qudit within an interval after injection and observing a sharp transition linked to the butterfly velocity.

Result: At strong coupling, the study found a sharp transition in quantum information correlation as a function of interval length, occurring at ℓ ∼ v_B T, directly linking the butterfly velocity to the emergence of locality.

Conclusion: The study concludes that the sharp light-cone emergence in quantum information spread is underpinned by a non-linear diffusion-like equation (FKPP), offering insights into the physical mechanisms behind operator growth in chaotic systems.

Abstract: The Ryu-Takayanagi (RT) formula and its interpretation in terms of quantum
error correction (QEC) implies an emergent locality for the spread of quantum
information in holographic CFTs, where information injected at a point in the
boundary theory spreads within a sharp light-cone corresponding to the
butterfly velocity. This emergent locality is a necessary condition for the
existence of a geometric bulk dual with an RT-like formula for entanglement
entropy. In this paper, we use tools from QEC to study the spread of quantum
information and the emergence of a sharp light-cone in an analytically
tractable model of chaotic dynamics, namely a one-dimensional Brownian SYK
chain. We start with an infinite temperature state in this model and inject a
qudit at time $t=0$ at some point $p$ on the chain. We then explicitly
calculate the amount of information of the qudit contained in an interval of
length $2\ell$ (centered around $p$) at some later time $t=T$. We find that at
strong coupling, this quantity shows a sharp transition as a function of $\ell$
from near zero to near maximal correlation. The transition occurs at $\ell \sim
v_B T$, with $v_B$ being the butterfly velocity. Underlying the emergence of
this sharp light-cone is a non-linear generalization of the diffusion equation
called the FKPP equation, which admits sharp domain wall solutions at late
times and strong coupling. These domain wall solutions can be understood on
physical grounds from properties of operator growth in chaotic systems.

</details>


### [27] [The CFT of Sen's Formulation of Chiral Gauge Fields](https://arxiv.org/abs/2508.00199)
*Chris Hull,Neil Lambert*

Main category: hep-th

TL;DR: The paper discusses the generalization of Sen's action for chiral bosons to a bi-metric formulation for 2k-form gauge fields in higher dimensions, showing its relation to conformal field theories and presenting a democratic action for p-form gauge fields.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the relation between Sen's action for chiral bosons and the theory of two chiral bosonic scalars as a 'bosonisation' and generalize it to higher dimensions for gauge fields.

Method: Theoretical analysis of Sen's action for chiral bosons and its generalization to bi-metric formulation for 2k-form gauge fields in higher dimensions.

Result: The paper shows that the standard vertex operators for chiral scalars are vertex and line operators in the Sen formulation, derives the Sen formulation of correlation functions in chiral scalar theory, and generalizes the action to a bi-metric formulation for 2k-form gauge fields in higher dimensions.

Conclusion: The paper concludes that Sen's action for chiral bosons can be generalized to a bi-metric formulation for 2k-form gauge fields in higher dimensions, which reduces to a conformal field theory with a BF-type action when metrics are equal, and presents a democratic action for p-form gauge fields in any dimension.

Abstract: Sen's action for chiral bosons in 2 dimensions describes two chiral scalars,
one of which couples to the physical metric and one of which couples to a flat
metric. It has a generalisation in which the flat metric is replaced by an
arbitrary second metric and so can be formulated on any curved world-sheet.
When the two metrics are equal, the theory reduces to a $\beta \gamma$ system,
giving a non-unitary $c=2$ conformal field theory. We argue that the relation
between this and the theory of two chiral bosonic scalars of the same chirality
can be viewed as a \lq bosonisation'. We show that the standard vertex
operators for the chiral scalars are vertex operators and line operators in the
Sen formulation and derive the formulation in the Sen theory of correlation
functions in the chiral scalar theory. The flat space Sen theory can be coupled
to two different world-sheet metrics in such a way that one scalar couples to
one metric and the other to the other metric, so obtaining the general
formulation with two metrics.
  In $d=4k+2$ dimensions, the bi-metric action for a $2k$-form gauge field with
self-dual field strength reduces, when the two metrics are equal, to a
conformal field theory with a $BF$-type action, except that $B$ is a self-dual
$d/2$-form and $F$ is a $d/2$-form field strength, $F=dP$. The self-duality of
$B$ means that this is not a topological theory but instead represents two
self-dual gauge fields. This has a generalisation to a democratic action for
$p$-form gauge fields in any dimension.

</details>


### [28] [Holographic Wilson Loop One-point Functions in ABJM Theory](https://arxiv.org/abs/2508.00281)
*Xiao-Yi Zhang,Yunfeng Jiang,Jun-Bao Wu*

Main category: hep-th

TL;DR: The study computes correlation functions in ABJM theory using M-theory and AdS/CFT correspondence, showing agreement with supersymmetric localization results in the large-$N$ limit.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from understanding the correspondence between holographic calculations in M-theory and results obtained via supersymmetric localization in ABJM theory, particularly for correlation functions involving Wilson loops and local operators.

Method: The authors use the AdS/CFT correspondence to compute correlation functions, treating them as dual to fluctuations of a probe M2-brane in $AdS_4 \times S^7/\mathbb{Z}_k$, and derive analytic results which are compared to existing localization results.

Result: Analytic results for correlation functions are derived for cases involving a circular half-BPS Wilson loop (or straight Wilson line) and local operators like the $1/3$-BPS single-trace chiral primary operator or the stress-energy tensor, showing agreement with localization results.

Conclusion: The holographic results for the correlation functions between Wilson loops and local operators in ABJM theory align perfectly with those obtained from supersymmetric localization in the large-$N$ limit with finite $k$.

Abstract: We compute the correlation function between a circular half-BPS Wilson loop
(or straight Wilson line) and a local operator in ABJM theory utilizing its
M-theory description. The local operator can be a $1/3$-BPS single-trace chiral
primary operator or the stress-energy tensor. Using the AdS/CFT correspondence,
these correlators are dual to fluctuations of a probe M2-brane in $AdS_4 \times
S^7/\mathbb{Z}_k$. We derive analytic results for both cases and compare them
with existing results based on supersymmetric localization in the literature.
In the large-$N$ limit with $k$ finite, our holograkphic results exhibit
perfect agreement with localization.

</details>


### [29] [Quasi-Normal Modes and Nonlinear Electrodynamics in Black Hole Phase Transitions](https://arxiv.org/abs/2508.00404)
*Zi-Yu Hou,Yu-Qi Lei,Xian-Hui Ge*

Main category: hep-th

TL;DR: This paper explores the connection between thermodynamic phase transitions and quasi-normal modes in black holes, showing that dynamical features like QNMs reflect thermodynamic changes, revealing a fundamental link between the two.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore whether thermodynamic phase transitions in black holes are connected to their dynamical properties, specifically through quasi-normal modes (QNMs) in the presence of nonlinear electromagnetic fields.

Method: The researchers analyzed the QNMs spectrum of massless scalar fields in charged black holes under $F(R)$-Euler-Heisenberg gravity, particularly focusing on the slope parameter $K$ and its alignment with thermodynamic phase transitions described by heat capacity.

Result: The study found that the transition point for the disappearance of divergence in the QNMs slope parameter aligns with changes in the thermodynamic phase structure, robust under variations of curvature and charge. However, larger angular quantum numbers reduce this correspondence, while higher overtone numbers restore it beyond a threshold.

Conclusion: The study concludes that there is a fundamental link between thermodynamic phase transitions and dynamical properties in black holes, as demonstrated by the correspondence between thermodynamic phase structures and quasi-normal modes (QNMs).

Abstract: We investigate the connection between thermodynamic phase transitions and
quasi-normal modes (QNMs) in charged black holes with a positive curvature
constant, within the framework of $F(R)$-Euler-Heisenberg gravity. Nonlinear
electromagnetic fields lead to rich thermodynamic phase structures and
significantly affect the QNMs of massless scalar fields. By analyzing the QNMs
spectrum, we find that the transition point marking the disappearance of
divergence in the QNMs slope parameter $K$ aligns with the change of the
thermodynamic phase structure described by the heat capacity, within the bounds
of computational uncertainty. This precise matching holds under variations of
curvature parameters and charge. Furthermore, we show that larger angular
quantum number $l$ diminishes this correspondence, while higher overtone number
$n$ restores it beyond a threshold. These findings demonstrate that
thermodynamic phase transitions of black holes carry embedded dynamical
information, uncovering a fundamental link between black hole thermodynamic and
dynamical properties.

</details>


### [30] [Aspects of 4d $\mathcal{N}=1$ $ADE$ gauge theories from M-theory: decomposition, automorphisms, and generalised symmetries](https://arxiv.org/abs/2508.00564)
*Osama Khlaif,Marwan Najjar*

Main category: hep-th

TL;DR: This paper explores the decomposition and symmetries of 4d N=1 gauge theories using M-theory, revealing modified instanton sums and higher 4-group structures.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the decomposition structure and symmetries of 4d N=1 gauge theories with specific Lie algebras through M-theory.

Method: The method involves M-theory geometric engineering and the analysis of quotienting the Bryant-Salamon spin bundle over the 3-sphere by finite subgroups.

Result: The result includes the identification of both inner and outer automorphisms in gauge theories, extending decomposition structures to various gauge algebras, and analyzing p-form symmetries.

Conclusion: The paper concludes that 4d N=1 gauge theories exhibit modified instanton sums and higher 4-group structures, which are derived from M-theory.

Abstract: We study the decomposition of 4d $\mathcal{N}=1$ gauge theories with Lie
algebras of type $\mathfrak{su}(N)$, $\mathfrak{so}(2N)$, and
$\mathfrak{e}_{6}$, realized via M-theory geometric engineering. These
theories, together with their novel decomposition structure, arise from
quotienting the Bryant--Salamon spin bundle over the 3-sphere by special finite
subgroups acting simultaneously on both the fiber and base. We show that these
gauge theories admit both inner and outer automorphisms, enabling sequences of
gauge theory breaking. In particular, outer automorphisms extend the
decomposition structure to theories with $\mathfrak{so}(2N+1)$,
$\mathfrak{sp}(2N)$, $\mathfrak{f}_{4}$, and $\mathfrak{g}_{2}$ gauge algebras.
For these theories, including both simply-laced and non-simply-laced cases, we
analyze their $p$-form symmetries, including $(-1)$-form symmetries, derive the
corresponding SymTFTs, and identify the M-theoretic origin of their symmetry
topological operators and defects. Finally, we demonstrate that these gauge
theories exhibit modified instanton sums and higher 4-group structures, and we
derive the associated topological sector directly from M-theory.

</details>


### [31] [Higher spin fields and the field strength multicopy](https://arxiv.org/abs/2508.00711)
*Graham R. Brown,Bill Spence*

Main category: hep-th

TL;DR: 本文研究了Weyl双副本向高自旋理论的推广，揭示了其与麦克斯韦张量的关系，并在不同维度和背景下进行了分析，尤其是在四维中表现得最为清晰。


<details>
  <summary>Details</summary>
Motivation: 探索高自旋理论的结构，尤其是如何将Weyl双副本推广到多副本，并研究其在不同维度中的表现。

Method: 使用Kerr-Schild坐标，通过自旋or描述和Penrose变换，分析高自旋场的迹条件和Fronsdal运动方程。

Result: 在四维中，多副本结构最为清晰，并揭示了高维自旋多副本中有趣的新特性。此外，讨论了反德西特背景下的向量空间形式主义。

Conclusion: 本文探讨了Weyl双副本向高自旋“多副本”的推广，并研究了如何将自然的线性高自旋场强度与麦克斯韦张量的幂和相关联。

Abstract: We discuss the generalisation of the Weyl double copy to higher spin
"multi-copies", showing how the natural linearised higher spin field strengths
can be related to sums of powers of the Maxwell tensor. The tracelessness of
the field strength involves the appropriate Fronsdal equations of motion for
the higher spin field. We work with spacetimes admitting Kerr-Schild
coordinates and give a number of examples in different dimensions. We note that
the multi-copy is particularly transparent in four dimensions if one uses
spinor descriptions of the fields, relating this to the Penrose transform. The
higher-dimensional spinor multicopy is also explored and reveals some
interesting new features arising from the little group based identification of
higher spin field strengths and Maxwell tensor types. We then turn to the
vector superspace formalism describing higher spin and `continuous' spin
representations given by Schuster and Toro, based on symmetric tensor fields.
Here the Kerr-Schild higher spin fields we have used earlier naturally package
into a simple expression involving an arbitrary function, when the continuous
spin scale $\rho$ is set to zero. Further, we discuss the case of an anti-de
Sitter background, where there is also a vector space formalism given by Segal
and we clarify this approach using a different definition of the covariant
derivative. We give a general solution of Kerr-Schild type and finally we
describe some of the obstacles to a continuous spin formulation.

</details>


### [32] [Gauge symmetry and radiatively induced terms in dimension-5 non-minimal Lorentz-violating QED](https://arxiv.org/abs/2508.00801)
*A. P. B. Scarpelli,A. R. Vieira*

Main category: hep-th

TL;DR: This paper explores the gauge invariance and induced terms in a non-minimal dimension-5 Lorentz-violating QED, finding that the symmetry conditions mirror those of standard QED.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the gauge invariance of a non-minimal dimension-5 Lorentz-violating QED and to explore the induced terms in this framework.

Method: The research involves deriving conditions for gauge invariance, computing two and three point functions at one-loop, and checking gauge Ward identities.

Result: The research finds that the non-minimal Lorentz-violating $a^{5}_F$-term in the fermion sector can radiatively induce a non-minimal Lorentz-violating term in the photon sector.

Conclusion: The study concludes that the conditions required to maintain gauge symmetry in the non-minimal dimension-5 Lorentz-violating QED are the same as those in usual QED.

Abstract: In this work, we derive the conditions that assure gauge invariance of a
non-minimal dimension-5 Lorentz-violating QED. The two and three point
functions at one-loop are computed. The gauge Ward identities are checked and
the conditions to assure gauge symmetry of this non-minimal framework is found
to be the same of the usual QED. Induced terms are also investigated and it is
shown that the non-minimal Lorentz-violating $a^{(5)}_F$-term of the fermion
sector can induce radiatively a non-minimal Lorentz-violating term in the
photon sector.

</details>


### [33] [Proper-time functional renormalization in $O(N)$ scalar models coupled to gravity](https://arxiv.org/abs/2508.00807)
*Alfio Bonanno,Emiliano Glaviano,Gian Paolo Vacca*

Main category: hep-th

TL;DR: 本文使用功能性的Wilsonian重整化群框架，结合适当的时间调节器，探索四维和三维中O(N)不变标量场多重态与引力耦合的缩放解和临界性质。


<details>
  <summary>Details</summary>
Motivation: 为了验证功能性Wilsonian重整化群框架在研究缩放解和临界性质中的有效性，并与已有的有效平均作用框架的结果进行比较。

Method: 采用背景-涨落分裂和规范固定程序，结合功能性Wilsonian重整化群框架和适当的时间调节器，对有效作用进行截断并分析其缩放解和临界指数。

Result: 大多数情况下，功能性Wilsonian重整化群框架的结果与有效平均作用框架的结果在定性和定量层面一致，但在有限N和大N极限下存在一些差异。

Conclusion: 功能性Wilsonian重整化群框架在研究标量场与引力耦合的临界性质方面是有效的，尽管结果在某些情况下与传统框架存在差异。

Abstract: We focus on the use of the functional Wilsonian renormalization group
framework characterized by a proper time regulator and test its use in the
search of the scaling solutions and the critical properties of an
O(N)-invariant scalar field multiplet coupled to gravity in d=4 and d=3
dimensions. We employ the same background-fluctuation splitting and gauge
fixing procedure, already adopted in a previous study based, instead, on the
effective average action framework and a similar truncation of the effective
action. Our main goal is to compare the results for the scaling solutions and
some of the associated critical exponents. In this analysis, performed in a
different framework, most of the picture previously uncovered is confirmed both
at qualitative and quantitative level. There are, neverthelss, few differences
both at finite N and in its large value limit, depending also on the schemes
which in both frameworks are called 'improved'

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [34] [Rethinking Evidence Hierarchies in Medical Language Benchmarks: A Critical Evaluation of HealthBench](https://arxiv.org/abs/2508.00081)
*Fred Mutisya,Shikoh Gitau,Nasubo Ongoma,Keith Mbae,Elizabeth Wamicha*

Main category: cs.AI

TL;DR: 本文旨在通过将评估标准与临床实践指南结合，提升医学语言模型评估的全球相关性和公平性。


<details>
  <summary>Details</summary>
Motivation: HealthBench依赖专家意见而非高级临床证据，可能在低收入和中等收入环境中加剧区域偏见和个体临床医生的差异，因此需要更具全球相关性和公平性的评估基准。

Method: 提出了一条通过“基于证据的强化学习”路线，包括将评分标准与指南关联、证据加权评分和情境覆盖逻辑，并关注伦理考虑和延迟结果反馈的整合。

Result: 提出了一种基于系统评价和GRADE证据评级的版本控制临床实践指南（CPGs）支持的“证据-稳健”强化学习方法，以解决HealthBench的局限性。

Conclusion: 通过将奖励函数锚定在经过严格审查的临床实践指南（CPGs）中，同时保持HealthBench的透明度和医生参与度，我们旨在培养不仅语言流畅而且临床可信、伦理健全且具有全球相关性的医学语言模型。

Abstract: HealthBench, a benchmark designed to measure the capabilities of AI systems
for health better (Arora et al., 2025), has advanced medical language model
evaluation through physician-crafted dialogues and transparent rubrics.
However, its reliance on expert opinion, rather than high-tier clinical
evidence, risks codifying regional biases and individual clinician
idiosyncrasies, further compounded by potential biases in automated grading
systems. These limitations are particularly magnified in low- and middle-income
settings, where issues like sparse neglected tropical disease coverage and
region-specific guideline mismatches are prevalent.
  The unique challenges of the African context, including data scarcity,
inadequate infrastructure, and nascent regulatory frameworks, underscore the
urgent need for more globally relevant and equitable benchmarks. To address
these shortcomings, we propose anchoring reward functions in version-controlled
Clinical Practice Guidelines (CPGs) that incorporate systematic reviews and
GRADE evidence ratings.
  Our roadmap outlines "evidence-robust" reinforcement learning via
rubric-to-guideline linkage, evidence-weighted scoring, and contextual override
logic, complemented by a focus on ethical considerations and the integration of
delayed outcome feedback. By re-grounding rewards in rigorously vetted CPGs,
while preserving HealthBench's transparency and physician engagement, we aim to
foster medical language models that are not only linguistically polished but
also clinically trustworthy, ethically sound, and globally relevant.

</details>


### [35] [Hyperproperty-Constrained Secure Reinforcement Learning](https://arxiv.org/abs/2508.00106)
*Ernest Bonnah,Luan Viet Nguyen,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: This paper introduces a security-aware reinforcement learning method constrained by HyperTWTL specifications, showing improved performance in a robotic application.


<details>
  <summary>Details</summary>
Motivation: There is a research gap in exploring security-aware reinforcement learning (RL) using hyperproperties, specifically in applying temporal logic constraints to secure RL in robotics.

Method: The authors formalize security and opacity constraints using HyperTWTL and apply dynamic Boltzmann softmax RL to learn security-aware optimal policies while satisfying these constraints.

Result: The approach was demonstrated effective through a pick-up and delivery robotic mission case study, with performance superior to two baseline RL algorithms.

Conclusion: The paper concludes that their proposed method of HyperTWTL-constrained secure reinforcement learning (SecRL) is effective and scalable, outperforming other baseline RL algorithms in a robotic mission case study.

Abstract: Hyperproperties for Time Window Temporal Logic (HyperTWTL) is a
domain-specific formal specification language known for its effectiveness in
compactly representing security, opacity, and concurrency properties for
robotics applications. This paper focuses on HyperTWTL-constrained secure
reinforcement learning (SecRL). Although temporal logic-constrained safe
reinforcement learning (SRL) is an evolving research problem with several
existing literature, there is a significant research gap in exploring
security-aware reinforcement learning (RL) using hyperproperties. Given the
dynamics of an agent as a Markov Decision Process (MDP) and opacity/security
constraints formalized as HyperTWTL, we propose an approach for learning
security-aware optimal policies using dynamic Boltzmann softmax RL while
satisfying the HyperTWTL constraints. The effectiveness and scalability of our
proposed approach are demonstrated using a pick-up and delivery robotic mission
case study. We also compare our results with two other baseline RL algorithms,
showing that our proposed method outperforms them.

</details>


### [36] [No AI Without PI! Object-Centric Process Mining as the Enabler for Generative, Predictive, and Prescriptive Artificial Intelligence](https://arxiv.org/abs/2508.00116)
*Wil M. P. van der Aalst*

Main category: cs.AI

TL;DR: This paper argues that for AI to succeed in industrial settings, it must be grounded in Process Intelligence (PI), particularly through Object-Centric Process Mining (OCPM), which connects data and processes and enables generative, predictive, and prescriptive AI applications.


<details>
  <summary>Details</summary>
Motivation: Organizations struggle to successfully implement AI in industrial settings due to the complexity and dynamic nature of operational processes, prompting the need for a process-centric approach like Process Intelligence.

Method: The paper discusses the application of Object-Centric Process Mining (OCPM) as a foundational approach for integrating generative, predictive, and prescriptive AI in operational processes.

Result: The paper demonstrates that Process Intelligence (PI) enables AI to handle diverse object and event types, bridging the gap between data and processes and facilitating the effective use of AI in organizations.

Conclusion: The paper concludes that AI must be grounded in Process Intelligence (PI), particularly through Object-Centric Process Mining (OCPM), to effectively improve operational processes in industrial settings.

Abstract: The uptake of Artificial Intelligence (AI) impacts the way we work, interact,
do business, and conduct research. However, organizations struggle to apply AI
successfully in industrial settings where the focus is on end-to-end
operational processes. Here, we consider generative, predictive, and
prescriptive AI and elaborate on the challenges of diagnosing and improving
such processes. We show that AI needs to be grounded using Object-Centric
Process Mining (OCPM). Process-related data are structured and
organization-specific and, unlike text, processes are often highly dynamic.
OCPM is the missing link connecting data and processes and enables different
forms of AI. We use the term Process Intelligence (PI) to refer to the
amalgamation of process-centric data-driven techniques able to deal with a
variety of object and event types, enabling AI in an organizational context.
This paper explains why AI requires PI to improve operational processes and
highlights opportunities for successfully combining OCPM and generative,
predictive, and prescriptive AI.

</details>


### [37] [Algorithmic Detection of Rank Reversals, Transitivity Violations, and Decomposition Inconsistencies in Multi-Criteria Decision Analysis](https://arxiv.org/abs/2508.00129)
*Agustín Borda,Juan Bautista Cabral,Gonzalo Giarda,Diego Nicolás Gimenez Irusta,Paula Pacheco,Alvaro Roy Schachner*

Main category: cs.AI

TL;DR: This paper introduces three tests to detect Rank Reversals in Multi-Criteria Decision Analysis and their implementation in the Scikit-Criteria library, aiming to improve the evaluation of decision methods.


<details>
  <summary>Details</summary>
Motivation: Rank Reversals are a serious problem in Multi-Criteria Decision Analysis that can significantly affect the results, so a mechanism to measure the performance of methods against alternatives is needed.

Method: The paper presents three tests to detect Rank Reversals and discusses their implementation in the Scikit-Criteria library, along with addressing complications and design considerations.

Result: Three tests were developed to detect Rank Reversals, and they were implemented in the Scikit-Criteria library while addressing complications for general scenarios.

Conclusion: The paper concludes that the proposed tests can play a major role in evaluating multi-criteria decision methods for problem-solving.

Abstract: In Multi-Criteria Decision Analysis, Rank Reversals are a serious problem
that can greatly affect the results of a Multi-Criteria Decision Method against
a particular set of alternatives. It is therefore useful to have a mechanism
that allows one to measure the performance of a method on a set of
alternatives. This idea could be taken further to build a global ranking of the
effectiveness of different methods to solve a problem. In this paper, we
present three tests that detect the presence of Rank Reversals, along with
their implementation in the Scikit-Criteria library. We also address the
complications that arise when implementing these tests for general scenarios
and the design considerations we made to handle them. We close with a
discussion about how these additions could play a major role in the judgment of
multi-criteria decision methods for problem solving.

</details>


### [38] [SHACL Validation under Graph Updates (Extended Paper)](https://arxiv.org/abs/2508.00137)
*Shqiponja Ahmetaj,George Konstantinidis,Magdalena Ortiz,Paolo Pareti,Mantas Simkus*

Main category: cs.AI

TL;DR: This paper proposes a method to validate SHACL constraints in evolving RDF graphs by reducing the problem to constraint (un)satisfiability, and presents a prototype implementation for static analysis.


<details>
  <summary>Details</summary>
Motivation: The motivation is to study SHACL validation in RDF graphs under updates and to develop a method for verifying that RDF graphs satisfying a SHACL specification continue to do so after modifications, which supports reasoning about evolving graphs.

Method: The authors use a regression technique that embeds update actions into SHACL constraints to reduce static validation under updates to constraint (un)satisfiability in SHACL. They also present a prototype implementation and evaluate it through experiments.

Result: The paper shows that static validation under SHACL updates can be reduced to constraint (un)satisfiability problems in SHACL. It also analyzes the computational complexity of this problem and demonstrates the behavior of a prototype implementation through experiments.

Conclusion: The paper concludes that static validation under updates can be reduced to (un)satisfiability of constraints in SHACL through a regression technique, and presents a prototype implementation for performing such validations.

Abstract: SHACL (SHApe Constraint Language) is a W3C standardized constraint language
for RDF graphs. In this paper, we study SHACL validation in RDF graphs under
updates. We present a SHACL-based update language that can capture intuitive
and realistic modifications on RDF graphs and study the problem of static
validation under such updates. This problem asks to verify whether every graph
that validates a SHACL specification will still do so after applying a given
update sequence. More importantly, it provides a basis for further services for
reasoning about evolving RDF graphs. Using a regression technique that embeds
the update actions into SHACL constraints, we show that static validation under
updates can be reduced to (un)satisfiability of constraints in (a minor
extension of) SHACL. We analyze the computational complexity of the static
validation problem for SHACL and some key fragments. Finally, we present a
prototype implementation that performs static validation and other static
analysis tasks on SHACL constraints and demonstrate its behavior through
preliminary experiments.

</details>


### [39] [Co-Producing AI: Toward an Augmented, Participatory Lifecycle](https://arxiv.org/abs/2508.00138)
*Rashid Mushkani,Hugo Berard,Toumadher Ammar,Cassandre Chatonnier,Shin Koseki*

Main category: cs.AI

TL;DR: This paper proposes a re-designed AI production pipeline that centers co-production, diversity, equity, inclusion, and collaboration to mitigate the disproportionate impacts of AI on marginalized groups.


<details>
  <summary>Details</summary>
Motivation: AI algorithms can disproportionately impact culturally marginalized groups, and current approaches to address these risks are insufficient.

Method: The paper draws on design justice, expansive learning theory, and empirical work on participatory AI, informed by four multidisciplinary workshops.

Result: An augmented AI lifecycle with five interconnected phases (co-framing, co-design, co-implementation, co-deployment, and co-maintenance) is introduced, along with connections to ethical frameworks and research questions for scaling participatory governance.

Conclusion: The paper concludes that mitigating the harms of AI algorithms requires a fundamental re-architecture of the AI production pipeline, emphasizing co-production, DEI, and multidisciplinary collaboration.

Abstract: Despite efforts to mitigate the inherent risks and biases of artificial
intelligence (AI) algorithms, these algorithms can disproportionately impact
culturally marginalized groups. A range of approaches has been proposed to
address or reduce these risks, including the development of ethical guidelines
and principles for responsible AI, as well as technical solutions that promote
algorithmic fairness. Drawing on design justice, expansive learning theory, and
recent empirical work on participatory AI, we argue that mitigating these harms
requires a fundamental re-architecture of the AI production pipeline. This
re-design should center co-production, diversity, equity, inclusion (DEI), and
multidisciplinary collaboration. We introduce an augmented AI lifecycle
consisting of five interconnected phases: co-framing, co-design,
co-implementation, co-deployment, and co-maintenance. The lifecycle is informed
by four multidisciplinary workshops and grounded in themes of distributed
authority and iterative knowledge exchange. Finally, we relate the proposed
lifecycle to several leading ethical frameworks and outline key research
questions that remain for scaling participatory governance.

</details>


### [40] [Beyond Agreement: Rethinking Ground Truth in Educational AI Annotation](https://arxiv.org/abs/2508.00143)
*Danielle R. Thomas,Conrad Borchers,Kenneth R. Koedinger*

Main category: cs.AI

TL;DR: 该论文认为，过度依赖人类IRR作为注释质量的守门员会阻碍教育数据分类的进步，提出应优先考虑有效性和教育影响而非一致性。


<details>
  <summary>Details</summary>
Motivation: 由于人类评估者存在偏见和不可靠性，过度依赖传统IRR指标可能阻碍教育数据分类和预测学习效果的进步。

Method: 该论文通过分析传统IRR指标的局限性，提出五种互补的评估方法，并强调外部有效性的重要性。

Result: 该论文指出，多标签注释方案、基于专家的方法和闭环有效性等方法能够产生更有效的训练数据和模型，从而提高学生学习效果和提供更有价值的见解。

Conclusion: 该论文呼吁重新思考注释质量和真实情况，优先考虑有效性而非一致性，以改进教育数据分类和模型构建。

Abstract: Humans can be notoriously imperfect evaluators. They are often biased,
unreliable, and unfit to define "ground truth." Yet, given the surging need to
produce large amounts of training data in educational applications using AI,
traditional inter-rater reliability (IRR) metrics like Cohen's kappa remain
central to validating labeled data. IRR remains a cornerstone of many machine
learning pipelines for educational data. Take, for example, the classification
of tutors' moves in dialogues or labeling open responses in machine-graded
assessments. This position paper argues that overreliance on human IRR as a
gatekeeper for annotation quality hampers progress in classifying data in ways
that are valid and predictive in relation to improving learning. To address
this issue, we highlight five examples of complementary evaluation methods,
such as multi-label annotation schemes, expert-based approaches, and
close-the-loop validity. We argue that these approaches are in a better
position to produce training data and subsequent models that produce improved
student learning and more actionable insights than IRR approaches alone. We
also emphasize the importance of external validity, for example, by
establishing a procedure of validating tutor moves and demonstrating that it
works across many categories of tutor actions (e.g., providing hints). We call
on the field to rethink annotation quality and ground truth--prioritizing
validity and educational impact over consensus alone.

</details>


### [41] [Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power](https://arxiv.org/abs/2508.00159)
*Jobst Heitzig,Ram Potham*

Main category: cs.AI

TL;DR: This paper proposes a novel objective function for AI systems that aims to empower humans by managing power dynamics safely, potentially enhancing both AI safety and human wellbeing.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the importance of power dynamics in AI safety, including power-seeking behavior, human disempowerment, and the need to maintain a desirable power balance between humans and AI agents. The goal is to enhance both safety and wellbeing.

Method: The authors use a principled, partially axiomatic approach to design an objective function that captures human power, incorporating bounded rationality, social norms, and diverse human goals. They derive algorithms to compute or approximate this metric using backward induction and multi-agent reinforcement learning.

Result: The authors developed a parametrizable and decomposable objective function that represents a risk- and inequality-averse aggregation of human power and derived algorithms to compute or approximate it. They demonstrated its implications in various scenarios and identified potential instrumental sub-goals.

Conclusion: The paper concludes that softly maximizing aggregate metrics of human power can be a safer and beneficial objective for AI systems compared to utility-based objectives.

Abstract: Power is a key concept in AI safety: power-seeking as an instrumental goal,
sudden or gradual disempowerment of humans, power balance in human-AI
interaction and international AI governance. At the same time, power as the
ability to pursue diverse goals is essential for wellbeing.
  This paper explores the idea of promoting both safety and wellbeing by
forcing AI agents explicitly to empower humans and to manage the power balance
between humans and AI agents in a desirable way. Using a principled, partially
axiomatic approach, we design a parametrizable and decomposable objective
function that represents an inequality- and risk-averse long-term aggregate of
human power. It takes into account humans' bounded rationality and social
norms, and, crucially, considers a wide variety of possible human goals.
  We derive algorithms for computing that metric by backward induction or
approximating it via a form of multi-agent reinforcement learning from a given
world model. We exemplify the consequences of (softly) maximizing this metric
in a variety of paradigmatic situations and describe what instrumental
sub-goals it will likely imply. Our cautious assessment is that softly
maximizing suitable aggregate metrics of human power might constitute a
beneficial objective for agentic AI systems that is safer than direct
utility-based objectives.

</details>


### [42] [RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization](https://arxiv.org/abs/2508.00222)
*Yihong Dong,Xue Jiang,Yongding Tao,Huanyu Liu,Kechi Zhang,Lili Mou,Rongyu Cao,Yingwei Ma,Jue Chen,Binhua Li,Zhi Jin,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.AI

TL;DR: 本文提出RL-PLUS方法，通过结合内部探索与外部数据，在多个推理任务中显著提升大型语言模型的表现，并解决了能力边界崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR方法受限于基础LLM的能力边界，且可能导致能力边界崩溃，限制了问题解决范围。

Method: RL-PLUS结合了内部探索（思考）和外部数据（学习），引入了多重重要性采样和基于探索的优势函数。

Result: RL-PLUS在六个数学推理基准测试和六个分布外推理任务中均达到最先进的性能，平均相对提升21.1%到69.2%。

Conclusion: RL-PLUS有效地解决了RLVR的能力边界崩溃问题，并在多个基准测试中展示了卓越的性能，突破了基础模型的限制。

Abstract: Reinforcement Learning with Verifiable Reward (RLVR) has significantly
advanced the complex reasoning abilities of Large Language Models (LLMs).
However, it struggles to break through the inherent capability boundaries of
the base LLM, due to its inherently on-policy strategy with LLM's immense
action space and sparse reward. Further, RLVR can lead to the capability
boundary collapse, narrowing the LLM's problem-solving scope. To address this
problem, we propose RL-PLUS, a novel approach that synergizes internal
exploitation (i.e., Thinking) with external data (i.e., Learning) to achieve
stronger reasoning capabilities and surpass the boundaries of base models.
RL-PLUS integrates two core components: Multiple Importance Sampling to address
for distributional mismatch from external data, and an Exploration-Based
Advantage Function to guide the model towards high-value, unexplored reasoning
paths. We provide both theoretical analysis and extensive experiments to
demonstrate the superiority and generalizability of our approach. The results
show that RL-PLUS achieves state-of-the-art performance compared with existing
RLVR methods on six math reasoning benchmarks and exhibits superior performance
on six out-of-distribution reasoning tasks. It also achieves consistent and
significant gains across diverse model families, with average relative
improvements ranging from 21.1\% to 69.2\%. Moreover, Pass@k curves across
multiple benchmarks indicate that RL-PLUS effectively resolves the capability
boundary collapse problem.

</details>


### [43] [MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning](https://arxiv.org/abs/2508.00271)
*Hongjin Qian,Zheng Liu*

Main category: cs.AI

TL;DR: MetaAgent是一种自我演化的代理系统，通过动态整合外部工具和知识库，在知识发现任务中表现出色，无需参数调整。


<details>
  <summary>Details</summary>
Motivation: 开发一种基于实践和持续自我改进的代理系统，以增强知识发现能力，减少对模型参数调整和后训练的依赖。

Method: MetaAgent通过生成自然语言帮助请求，结合工具路由机制，利用外部工具解决知识差距，并通过自我反思和答案验证持续改进推理和工具使用策略。

Result: 在GAIA、WebWalkerQA和BrowseCamp等知识发现基准测试中，MetaAgent始终优于基于工作流的基线模型，并与端到端训练的代理相匹配或超越。

Conclusion: MetaAgent是一个自我进化、基于代理的系统，具有强大的通用知识发现能力，不需要改变模型参数或进一步后训练。

Abstract: In this work, we propose MetaAgent, an agentic paradigm inspired by the
principle of learning-by-doing, where expertise is developed through hands-on
practice and continual self-improvement. MetaAgent starts with a minimal
workflow, equipped only with basic reasoning and adaptive help-seeking
abilities. When a knowledge gap is encountered, MetaAgent generates natural
language help requests, which are routed to the most suitable external tool by
a dedicated tool router. As MetaAgent solves tasks, it continually conducts
self-reflection and answer verification, distilling actionable experience into
concise texts that are dynamically incorporated into future task contexts.
Besides, MetaAgent autonomously builds in-house tools and a persistent
knowledge base by organizing its tool-use history, further enhancing its
ability to retrieve and integrate relevant information We term this continual,
data-driven process as \textit{meta tool learning}, through which MetaAgent
incrementally refines its reasoning and tool-use strategies, without changing
model parameters or requiring further post-training. Evaluated on challenging
knowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp,
MetaAgent consistently outperforms workflow-based baselines and matches or
exceeds end-to-end trained agents, demonstrating the promise of self-evolving
agentic systems for robust, general-purpose knowledge discovery. We provide our
source codes in https://github.com/qhjqhj00/MetaAgent.

</details>


### [44] [Mind the Gap: The Divergence Between Human and LLM-Generated Tasks](https://arxiv.org/abs/2508.00282)
*Yi-Long Lu,Jiajun Song,Chunhui Zhang,Wei Wang*

Main category: cs.AI

TL;DR: This study reveals a disconnect between human cognition and LLMs in task generation, emphasizing the need to incorporate intrinsic motivation and physical grounding in AI agents for more human-like behavior.


<details>
  <summary>Details</summary>
Motivation: To determine whether generative agents powered by large language models (LLMs) operate on similar cognitive principles as humans, particularly regarding internal motivations and task generation.

Method: Conducted a task-generation experiment comparing human responses with those of an LLM agent (GPT-4o), explicitly testing the influence of psychological drivers such as personal values and cognitive style.

Result: Human task generation was consistently influenced by psychological drivers, while the LLM failed to reflect corresponding behavioral patterns even when explicitly provided with these drivers. LLM-generated tasks were less social, less physical, and thematically biased toward abstraction, despite being perceived as more fun and novel.

Conclusion: There is a core gap between the value-driven, embodied nature of human cognition and the statistical patterns of LLMs, highlighting the necessity of incorporating intrinsic motivation and physical grounding into the design of more human-aligned agents.

Abstract: Humans constantly generate a diverse range of tasks guided by internal
motivations. While generative agents powered by large language models (LLMs)
aim to simulate this complex behavior, it remains uncertain whether they
operate on similar cognitive principles. To address this, we conducted a
task-generation experiment comparing human responses with those of an LLM agent
(GPT-4o). We find that human task generation is consistently influenced by
psychological drivers, including personal values (e.g., Openness to Change) and
cognitive style. Even when these psychological drivers are explicitly provided
to the LLM, it fails to reflect the corresponding behavioral patterns. They
produce tasks that are markedly less social, less physical, and thematically
biased toward abstraction. Interestingly, while the LLM's tasks were perceived
as more fun and novel, this highlights a disconnect between its linguistic
proficiency and its capacity to generate human-like, embodied goals.We conclude
that there is a core gap between the value-driven, embodied nature of human
cognition and the statistical patterns of LLMs, highlighting the necessity of
incorporating intrinsic motivation and physical grounding into the design of
more human-aligned agents.

</details>


### [45] [Oedipus and the Sphinx: Benchmarking and Improving Visual Language Models for Complex Graphic Reasoning](https://arxiv.org/abs/2508.00323)
*Jianyi Zhang,Xu Ji,Ziyin Zhou,Yuchen Zhou,Shubo Shi,Haoyu Wu,Zhen Li,Shizhao Liu*

Main category: cs.AI

TL;DR: This study introduces ReasonBench, a new benchmark for evaluating VLMs in complex graphic reasoning tasks, and proposes optimization strategies that significantly improve model performance.


<details>
  <summary>Details</summary>
Motivation: VLMs struggle with complex graphic reasoning and abstract problem solving, which are not adequately addressed in existing research.

Method: The researchers introduced ReasonBench, a benchmark for structured graphic reasoning tasks, and tested 11 mainstream VLMs. They proposed two optimization methods: DiaCoT for interpretability and ReasonTune for adaptability.

Result: Testing on ReasonBench revealed limitations in current VLMs, and the proposed optimization strategies improved performance by 33.5%.

Conclusion: The study concludes that current VLMs have significant limitations in complex graphic reasoning, but these can be improved through strategies like DiaCoT and ReasonTune.

Abstract: Evaluating the performance of visual language models (VLMs) in graphic
reasoning tasks has become an important research topic. However, VLMs still
show obvious deficiencies in simulating human-level graphic reasoning
capabilities, especially in complex graphic reasoning and abstract problem
solving, which are less studied and existing studies only focus on simple
graphics. To evaluate the performance of VLMs in complex graphic reasoning, we
propose ReasonBench, the first evaluation benchmark focused on structured
graphic reasoning tasks, which includes 1,613 questions from real-world
intelligence tests. ReasonBench covers reasoning dimensions related to
location, attribute, quantity, and multi-element tasks, providing a
comprehensive evaluation of the performance of VLMs in spatial, relational, and
abstract reasoning capabilities. We benchmark 11 mainstream VLMs (including
closed-source and open-source models) and reveal significant limitations of
current models. Based on these findings, we propose a dual optimization
strategy: Diagrammatic Reasoning Chain (DiaCoT) enhances the interpretability
of reasoning by decomposing layers, and ReasonTune enhances the task
adaptability of model reasoning through training, all of which improves VLM
performance by 33.5\%. All experimental data and code are in the repository:
https://huggingface.co/datasets/cistine/ReasonBench.

</details>


### [46] [R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge](https://arxiv.org/abs/2508.00324)
*Yeonjun In,Wonjoong Kim,Sangwu Park,Chanyoung Park*

Main category: cs.AI

TL;DR: 本文提出了一种名为R1-Act的后训练方法，通过触发模型已有的安全知识来提高大型推理模型的安全性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型推理模型在复杂任务上表现出色，但它们经常执行有害的用户指令，引发了重大的安全问题。

Method: 通过结构化的推理过程显式触发模型的安全知识。

Result: R1-Act在多个大型推理模型上的实验显示了其在改进安全性方面的强大效果，且仅需1000个训练示例和在单个RTX A6000 GPU上90分钟的训练时间。

Conclusion: R1-Act是一种简单高效的后训练方法，可以显著提高大型推理模型的安全性，同时保持推理性能。

Abstract: Although large reasoning models (LRMs) have demonstrated impressive
capabilities on complex tasks, recent studies reveal that these models
frequently fulfill harmful user instructions, raising significant safety
concerns. In this paper, we investigate the underlying cause of LRM safety
risks and find that models already possess sufficient safety knowledge but fail
to activate it during reasoning. Based on this insight, we propose R1-Act, a
simple and efficient post-training method that explicitly triggers safety
knowledge through a structured reasoning process. R1-Act achieves strong safety
improvements while preserving reasoning performance, outperforming prior
alignment methods. Notably, it requires only 1,000 training examples and 90
minutes of training on a single RTX A6000 GPU. Extensive experiments across
multiple LRM backbones and sizes demonstrate the robustness, scalability, and
practical efficiency of our approach.

</details>


### [47] [CoRGI: Verified Chain-of-Thought Reasoning with Visual Grounding](https://arxiv.org/abs/2508.00378)
*Shixin Yi,Lin Shang*

Main category: cs.AI

TL;DR: 本文提出CoRGI框架，通过在推理过程中引入视觉验证机制，有效提升视觉语言模型的推理准确性和解释质量。


<details>
  <summary>Details</summary>
Motivation: 现有的思维链（CoT）提示方法虽然提升了视觉语言模型的推理能力，但其生成的解释往往缺乏视觉内容的支持，因此需要引入视觉验证机制来减少错误。

Method: 提出了一种名为CoRGI的模块化框架，包含三个阶段：生成文本推理链、通过VEVM模块提取视觉证据、将文本推理与视觉证据结合生成验证答案。

Result: 在VCR基准测试中，CoRGI框架提升了Qwen-2.5VL和LLaVA-1.6等模型的推理性能，并通过消融实验和人工评估验证了其有效性。

Conclusion: 研究得出，在视觉语言模型中引入视觉验证步骤能够有效提升推理过程的准确性和解释的可靠性。

Abstract: Chain-of-Thought (CoT) prompting has shown promise in improving reasoning in
vision-language models (VLMs), but it often produces explanations that are
linguistically fluent yet lack grounding in visual content. We observe that
such hallucinations arise in part from the absence of an explicit verification
mechanism during multi-step reasoning. To address this, we propose
\textbf{CoRGI}(\textbf{C}hain \textbf{o}f \textbf{R}easoning with
\textbf{G}rounded \textbf{I}nsights), a modular framework that introduces
visual verification into the reasoning process. CoRGI follows a three-stage
pipeline: it first generates a textual reasoning chain, then extracts
supporting visual evidence for each reasoning step via a dedicated module
(VEVM), and finally synthesizes the textual rationale with visual evidence to
generate a grounded, verified answer. The framework can be integrated with
existing VLMs without end-to-end retraining. We evaluate CoRGI on the VCR
benchmark and find that it improves reasoning performance on two representative
open-source VLM backbones, Qwen-2.5VL and LLaVA-1.6. Ablation studies confirm
the contribution of each step in the verification module, and human evaluations
suggest that CoRGI leads to more factual and helpful explanations. We also
examine alternative designs for the visual verification step and discuss
potential limitations of post-hoc verification frameworks. These findings
highlight the importance of grounding intermediate reasoning steps in visual
evidence to enhance the robustness of multimodal reasoning.

</details>


### [48] [Theory of Mind Using Active Inference: A Framework for Multi-Agent Cooperation](https://arxiv.org/abs/2508.00401)
*Riddhi J. Pitliya,Ozan Catal,Toon Van de Maele,Corrado Pezzato,Tim Verbelen*

Main category: cs.AI

TL;DR: The paper presents a novel approach to multi-agent cooperation by implementing theory of mind within active inference, enabling agents to reason about others' beliefs while planning their own actions without relying on task-specific shared generative models or explicit communication.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve multi-agent cooperation by enabling agents to reason about others' beliefs while planning their own actions without relying on task-specific shared generative models or explicit communication.

Method: The paper implements theory of mind (ToM) within active inference and extends the inference tree-based planning algorithm to systematically explore joint policy spaces through recursive reasoning.

Result: Results demonstrate that ToM-equipped agents cooperate better compared to non-ToM counterparts by being able to avoid collisions and reduce redundant efforts.

Conclusion: The paper concludes that implementing theory of mind within active inference improves multi-agent cooperation, advancing practical applications in artificial intelligence and providing computational insights into ToM.

Abstract: We present a novel approach to multi-agent cooperation by implementing theory
of mind (ToM) within active inference. ToM - the ability to understand that
others can have differing knowledge and goals - enables agents to reason about
others' beliefs while planning their own actions. Unlike previous active
inference approaches to multi-agent cooperation, our method neither relies on
task-specific shared generative models nor requires explicit communication,
while being generalisable. In our framework, the ToM-equipped agent maintains
distinct representations of its own and others' beliefs and goals. We extend
the sophisticated inference tree-based planning algorithm to systematically
explore joint policy spaces through recursive reasoning. Our approach is
evaluated through collision avoidance and foraging task simulations. Results
demonstrate that ToM-equipped agents cooperate better compared to non-ToM
counterparts by being able to avoid collisions and reduce redundant efforts.
Crucially, ToM agents accomplish this by inferring others' beliefs solely from
observable behaviour. This work advances practical applications in artificial
intelligence while providing computational insights into ToM.

</details>


### [49] [Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training](https://arxiv.org/abs/2508.00414)
*Tianqing Fang,Zhisong Zhang,Xiaoyang Wang,Rui Wang,Can Qin,Yuxuan Wan,Jun-Yu Ma,Ce Zhang,Jiaqi Chen,Xiyun Li,Hongming Zhang,Haitao Mi,Dong Yu*

Main category: cs.AI

TL;DR: The paper introduces Cognitive Kernel-Pro, a fully open-source and free multi-module agent framework that achieves state-of-the-art results on GAIA, surpassing previous leading systems.


<details>
  <summary>Details</summary>
Motivation: Current agent systems are either closed-source or heavily reliant on paid APIs and proprietary tools, limiting accessibility and reproducibility. The authors aim to democratize the development and evaluation of advanced AI agents.

Method: The authors systematically investigated the curation of high-quality training data and explored novel strategies for agent test-time reflection and voting.

Result: Cognitive Kernel-Pro achieves state-of-the-art results among open-source and free agents, with an 8B-parameter model surpassing previous leading systems such as WebDancer and WebSailor.

Conclusion: Cognitive Kernel-Pro is a fully open-source and free multi-module agent framework that achieves state-of-the-art results on GAIA, surpassing previous leading systems.

Abstract: General AI Agents are increasingly recognized as foundational frameworks for
the next generation of artificial intelligence, enabling complex reasoning, web
interaction, coding, and autonomous research capabilities. However, current
agent systems are either closed-source or heavily reliant on a variety of paid
APIs and proprietary tools, limiting accessibility and reproducibility for the
research community. In this work, we present \textbf{Cognitive Kernel-Pro}, a
fully open-source and (to the maximum extent) free multi-module agent framework
designed to democratize the development and evaluation of advanced AI agents.
Within Cognitive Kernel-Pro, we systematically investigate the curation of
high-quality training data for Agent Foundation Models, focusing on the
construction of queries, trajectories, and verifiable answers across four key
domains: web, file, code, and general reasoning. Furthermore, we explore novel
strategies for agent test-time reflection and voting to enhance agent
robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving
state-of-the-art results among open-source and free agents. Notably, our
8B-parameter open-source model surpasses previous leading systems such as
WebDancer and WebSailor, establishing a new performance standard for
accessible, high-capability AI agents. Code is available at
https://github.com/Tencent/CognitiveKernel-Pro

</details>


### [50] [Thinking Machines: Mathematical Reasoning in the Age of LLMs](https://arxiv.org/abs/2508.00459)
*Andrea Asperti,Alberto Naibo,Claudio Sacerdoti Coen*

Main category: cs.AI

TL;DR: 本文探讨了大型语言模型在形式化数学推理中的挑战，分析了其与代码生成任务的差异，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在结构化推理和符号任务中表现出色，尤其在代码生成方面。然而，将其应用于形式化数学领域时面临显著困难。论文旨在探讨这一差异背后的原因，并分析模型的内部机制。

Method: 论文通过分析现有的模型和基准测试，探讨了形式化数学与非形式化数学作为训练领域的权衡、证明生成相较于代码合成更脆弱的原因，以及大型语言模型是否真正理解逻辑状态的演化。

Result: 论文总结了当前该领域的研究现状，明确了大型语言模型在形式化数学推理中的局限性，并提出了可能的改进方向。

Conclusion: 论文总结指出，尽管大型语言模型在代码生成等任务上表现出色，但在正式数学推理领域仍面临重大挑战。论文强调了当前模型的局限性，并探讨了如何突破这些限制以提升模型在数学推理方面的能力。

Abstract: Large Language Models (LLMs) have shown remarkable abilities in structured
reasoning and symbolic tasks, with coding emerging as a particular area of
strength. This success has sparked growing interest in applying LLMs to
mathematics, both in informal problem-solving and formal theorem proving.
However, progress in formal mathematics has proven to be significantly more
difficult, despite surface-level similarities between programming and proof
construction. This discrepancy raises important questions about how LLMs
``reason'', how they are supervised, and whether they internally track a notion
of computational or deductive state. In this article, we address the
state-of-the-art of the discipline, focusing on recent models and benchmarks,
and explore three central issues at the intersection of machine learning and
mathematical cognition: (i) the trade-offs between formal and informal
mathematics as training domains; (ii) the deeper reasons why proof generation
remains more brittle than code synthesis; (iii) and the question of whether
LLMs represent, or merely mimic, a notion of evolving logical state. Our goal
is not to draw hard boundaries, but to identify where the current limits lie,
and how they might be extended.

</details>


### [51] [Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking](https://arxiv.org/abs/2508.00500)
*Haoyu Wang,Chris M. Poskitt,Jun Sun,Jiali Wei*

Main category: cs.AI

TL;DR: 本文提出Pro2Guard，一种基于概率可达性分析的主动运行时安全执行框架，用于预测并干预大型语言模型代理的不安全行为，显著提高安全性和任务完成率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于规则的安全执行系统缺乏预见性，难以应对长视野依赖和分布变化，因此需要一种能够主动预测风险并提前干预的框架。

Method: 通过将代理行为抽象为符号状态，并从执行轨迹中学习离散时间马尔可夫链（DTMC），结合语义有效性检查和PAC边界，预测达到不安全状态的概率。

Result: 在具身代理任务中，Pro2Guard能够提前在93.6%的不安全任务中实施安全干预；在自动驾驶场景中，实现了100%的交通法规违规和碰撞预测，提前预判风险达38.66秒。

Conclusion: Pro2Guard是一个基于概率可达性分析的主动运行时执行框架，能够提前预测并干预不安全状态，有效提高了大型语言模型代理的安全性。

Abstract: Large Language Model (LLM) agents exhibit powerful autonomous capabilities
across domains such as robotics, virtual assistants, and web automation.
However, their stochastic behavior introduces significant safety risks that are
difficult to anticipate. Existing rule-based enforcement systems, such as
AgentSpec, focus on developing reactive safety rules, which typically respond
only when unsafe behavior is imminent or has already occurred. These systems
lack foresight and struggle with long-horizon dependencies and distribution
shifts. To address these limitations, we propose Pro2Guard, a proactive runtime
enforcement framework grounded in probabilistic reachability analysis.
Pro2Guard abstracts agent behaviors into symbolic states and learns a
Discrete-Time Markov Chain (DTMC) from execution traces. At runtime, it
anticipates future risks by estimating the probability of reaching unsafe
states, triggering interventions before violations occur when the predicted
risk exceeds a user-defined threshold. By incorporating semantic validity
checks and leveraging PAC bounds, Pro2Guard ensures statistical reliability
while approximating the underlying ground-truth model. We evaluate Pro2Guard
extensively across two safety-critical domains: embodied household agents and
autonomous vehicles. In embodied agent tasks, Pro2Guard enforces safety early
on up to 93.6% of unsafe tasks using low thresholds, while configurable modes
(e.g., reflect) allow balancing safety with task success, maintaining up to
80.4% task completion. In autonomous driving scenarios, Pro2Guard achieves 100%
prediction of traffic law violations and collisions, anticipating risks up to
38.66 seconds ahead.

</details>


### [52] [MultiSHAP: A Shapley-Based Framework for Explaining Cross-Modal Interactions in Multimodal AI Models](https://arxiv.org/abs/2508.00576)
*Zhanliang Wang,Kai Wang*

Main category: cs.AI

TL;DR: MultiSHAP is a model-agnostic interpretability framework that explains cross-modal interactions in multimodal AI models by attributing predictions to pairwise interactions between fine-grained visual and textual elements.


<details>
  <summary>Details</summary>
Motivation: Multimodal AI models are often considered 'black boxes,' limiting their deployment in high-stakes applications where interpretability is essential. Current methods cannot precisely quantify the synergistic effects between modalities or are limited to open-source models.

Method: MultiSHAP uses the Shapley Interaction Index to attribute predictions to pairwise interactions between fine-grained visual and textual elements.

Result: MultiSHAP successfully reveals synergistic and suppressive cross-modal effects at the instance level and uncovers generalizable interaction patterns across samples, as confirmed by experiments on public multimodal benchmarks and real-world case studies.

Conclusion: MultiSHAP provides a model-agnostic interpretability framework for explaining cross-modal interactions in multimodal AI models, offering both instance-level and dataset-level explanations.

Abstract: Multimodal AI models have achieved impressive performance in tasks that
require integrating information from multiple modalities, such as vision and
language. However, their "black-box" nature poses a major barrier to deployment
in high-stakes applications where interpretability and trustworthiness are
essential. How to explain cross-modal interactions in multimodal AI models
remains a major challenge. While existing model explanation methods, such as
attention map and Grad-CAM, offer coarse insights into cross-modal
relationships, they cannot precisely quantify the synergistic effects between
modalities, and are limited to open-source models with accessible internal
weights. Here we introduce MultiSHAP, a model-agnostic interpretability
framework that leverages the Shapley Interaction Index to attribute multimodal
predictions to pairwise interactions between fine-grained visual and textual
elements (such as image patches and text tokens), while being applicable to
both open- and closed-source models. Our approach provides: (1) instance-level
explanations that reveal synergistic and suppressive cross-modal effects for
individual samples - "why the model makes a specific prediction on this input",
and (2) dataset-level explanation that uncovers generalizable interaction
patterns across samples - "how the model integrates information across
modalities". Experiments on public multimodal benchmarks confirm that MultiSHAP
faithfully captures cross-modal reasoning mechanisms, while real-world case
studies demonstrate its practical utility. Our framework is extensible beyond
two modalities, offering a general solution for interpreting complex multimodal
AI models.

</details>


### [53] [From EMR Data to Clinical Insight: An LLM-Driven Framework for Automated Pre-Consultation Questionnaire Generation](https://arxiv.org/abs/2508.00581)
*Ruiqing Ding,Qianfang Sun,Yongkang Leng,Hui Yin,Xiaojian Li*

Main category: cs.AI

TL;DR: A novel multi-stage LLM-driven framework improves the generation of pre-consultation questionnaires from EMRs by building explicit clinical knowledge, outperforming direct approaches in key performance metrics.


<details>
  <summary>Details</summary>
Motivation: Generating comprehensive pre-consultation questionnaires from complex and voluminous Electronic Medical Records (EMRs) is challenging for direct Large Language Model (LLM) approaches due to issues in information completeness, logical order, and disease-level synthesis.

Method: The framework operates in three stages: (1) extracting atomic assertions from EMRs, (2) constructing personal causal networks and synthesizing disease knowledge through clustering, and (3) generating tailored and standardized questionnaires based on structured representations. The method was evaluated on a real-world EMR dataset and validated by clinical experts.

Result: The framework demonstrated superior performance in information coverage, diagnostic relevance, understandability, and generation time compared to direct methods.

Conclusion: The proposed multi-stage LLM-driven framework effectively overcomes limitations of direct methods in generating comprehensive pre-consultation questionnaires by building explicit clinical knowledge, demonstrating practical potential to enhance patient information collection.

Abstract: Pre-consultation is a critical component of effective healthcare delivery.
However, generating comprehensive pre-consultation questionnaires from complex,
voluminous Electronic Medical Records (EMRs) is a challenging task. Direct
Large Language Model (LLM) approaches face difficulties in this task,
particularly regarding information completeness, logical order, and
disease-level synthesis. To address this issue, we propose a novel multi-stage
LLM-driven framework: Stage 1 extracts atomic assertions (key facts with
timing) from EMRs; Stage 2 constructs personal causal networks and synthesizes
disease knowledge by clustering representative networks from an EMR corpus;
Stage 3 generates tailored personal and standardized disease-specific
questionnaires based on these structured representations. This framework
overcomes limitations of direct methods by building explicit clinical
knowledge. Evaluated on a real-world EMR dataset and validated by clinical
experts, our method demonstrates superior performance in information coverage,
diagnostic relevance, understandability, and generation time, highlighting its
practical potential to enhance patient information collection.

</details>


### [54] [Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings](https://arxiv.org/abs/2508.00632)
*Alexia Jolicoeur-Martineau*

Main category: cs.AI

TL;DR: 本文介绍了一种新的音视频内容质量评估指标AVR-Eval和一个基于此指标的多代理系统AVR-Agent，旨在解决当前AI在生成复杂交互式多媒体内容方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在生成文本、音频、图像和视频方面表现出色，但创建交互式音视频内容（如视频游戏）仍然具有挑战性。当前的LLMs可以生成JavaScript游戏和动画，但缺乏自动评估指标，并且难以处理通常需要团队合作数月才能完成的复杂内容（多轮、多代理）并使用艺术家制作的资源。为了解决这些问题，我们构建了一个新的指标和多代理系统。

Method: 我们提出了AVR-Eval，这是一种使用音视频录制（AVRs）的多媒体内容质量相对指标。一个全模态模型（处理文本、视频和音频）比较两个内容的AVRs，文本模型通过审查评估来确定优劣。我们还构建了AVR-Agent，这是一个多代理系统，从多媒体资源库（音频、图像、3D模型）生成JavaScript代码。

Result: 我们在游戏和动画上进行了实验，并使用AVR-Eval（内容A对B的胜率）进行评估。我们发现，通过AVR-Agent生成的内容相对于通过单次生成的内容具有显著更高的胜率。然而，模型难以有效利用自定义资源和AVR反馈，未显示更高的胜率。

Conclusion: 实验结果显示，虽然人类在高质量资源和视听反馈中受益，但目前的编码模型未能有效地利用这些资源，揭示了人类和机器内容创作方法之间的根本差异。

Abstract: While AI excels at generating text, audio, images, and videos, creating
interactive audio-visual content such as video games remains challenging.
Current LLMs can generate JavaScript games and animations, but lack automated
evaluation metrics and struggle with complex content that normally requires
teams of humans working for many months (multi-shot, multi-agents) using assets
made by artists. To tackle these issues, we built a new metric and a
multi-agent system.
  We propose AVR-Eval, a relative metric for multimedia content quality using
Audio-Visual Recordings (AVRs). An omni-modal model (processing text, video,
and audio) compares the AVRs of two contents, with a text model reviewing
evaluations to determine superiority. We show that AVR-Eval properly identifies
good from broken or mismatched content.
  We built AVR-Agent, a multi-agent system generating JavaScript code from a
bank of multimedia assets (audio, images, 3D models). The coding agent selects
relevant assets, generates multiple initial codes, uses AVR-Eval to identify
the best version, and iteratively improves it through omni-modal agent feedback
from the AVR.
  We run experiments on games and animations with AVR-Eval (win rate of content
A against B). We find that content generated by AVR-Agent has a significantly
higher win rate against content made through one-shot generation. However,
models struggle to leverage custom assets and AVR feedback effectively, showing
no higher win rate. This reveals a critical gap: while humans benefit from
high-quality assets and audio-visual feedback, current coding models do not
seem to utilize these resources as effectively, highlighting fundamental
differences between human and machine content creation approaches.

</details>


### [55] [Multi-Band Variable-Lag Granger Causality: A Unified Framework for Causal Time Series Inference across Frequencies](https://arxiv.org/abs/2508.00658)
*Chakattrai Sookkongwaree,Tattep Lakmuang,Chainarong Amornbunchornvej*

Main category: cs.AI

TL;DR: 本研究提出了多频段可变滞后格兰杰因果分析（MB-VLGC），以更准确地建模时间序列中因果关系的频率依赖性与可变时间滞后。


<details>
  <summary>Details</summary>
Motivation: 传统格兰杰因果分析中的固定滞后假设在复杂系统中往往不现实，而现有可变滞后方法未能考虑因果关系的频率依赖性。

Method: 提出多频段可变滞后格兰杰因果关系（MB-VLGC）框架，综合建模不同频率带的因果延迟，提供形式化定义和高效的推断流程。

Result: 实验表明，MB-VLGC在合成数据集和真实世界数据集上均显著优于现有方法，验证了其理论有效性和广泛适用性。

Conclusion: 多频段可变滞后格兰杰因果关系（MB-VLGC）框架在综合考虑时间延迟和频率依赖性方面优于现有方法，并在多个领域的时间序列数据中展现出广泛的应用潜力。

Abstract: Understanding causal relationships in time series is fundamental to many
domains, including neuroscience, economics, and behavioral science. Granger
causality is one of the well-known techniques for inferring causality in time
series. Typically, Granger causality frameworks have a strong fix-lag
assumption between cause and effect, which is often unrealistic in complex
systems. While recent work on variable-lag Granger causality (VLGC) addresses
this limitation by allowing a cause to influence an effect with different time
lags at each time point, it fails to account for the fact that causal
interactions may vary not only in time delay but also across frequency bands.
For example, in brain signals, alpha-band activity may influence another region
with a shorter delay than slower delta-band oscillations. In this work, we
formalize Multi-Band Variable-Lag Granger Causality (MB-VLGC) and propose a
novel framework that generalizes traditional VLGC by explicitly modeling
frequency-dependent causal delays. We provide a formal definition of MB-VLGC,
demonstrate its theoretical soundness, and propose an efficient inference
pipeline. Extensive experiments across multiple domains demonstrate that our
framework significantly outperforms existing methods on both synthetic and
real-world datasets, confirming its broad applicability to any type of time
series data. Code and datasets are publicly available.

</details>


### [56] [Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI](https://arxiv.org/abs/2508.00665)
*Maryam Mosleh,Marie Devlin,Ellis Solaiman*

Main category: cs.AI

TL;DR: 论文旨在通过结合传统XAI和生成式AI模型，以及用户个性化来提高教育领域人工智能系统的透明度和用户体验。


<details>
  <summary>Details</summary>
Motivation: 当前许多人工智能驱动的自适应学习系统缺乏透明度，且大多数可解释AI技术关注技术输出而忽视用户角色和理解。

Method: 重新定义可解释性为一种动态的沟通过程，并概述了框架的设计、关键XAI局限性以及关于准确性、公平性和个性化的研究方向。

Result: 提出了一个整合传统XAI技术、生成式AI模型和用户个性化的框架，以生成根据用户需求定制的个性化解释。

Conclusion: 该论文提出了一种混合框架，结合了传统XAI技术和生成式AI模型以及用户个性化，以生成多模态、个性化的解释，旨在增强透明度并支持以用户为中心的体验。

Abstract: Artificial intelligence-driven adaptive learning systems are reshaping
education through data-driven adaptation of learning experiences. Yet many of
these systems lack transparency, offering limited insight into how decisions
are made. Most explainable AI (XAI) techniques focus on technical outputs but
neglect user roles and comprehension. This paper proposes a hybrid framework
that integrates traditional XAI techniques with generative AI models and user
personalisation to generate multimodal, personalised explanations tailored to
user needs. We redefine explainability as a dynamic communication process
tailored to user roles and learning goals. We outline the framework's design,
key XAI limitations in education, and research directions on accuracy,
fairness, and personalisation. Our aim is to move towards explainable AI that
enhances transparency while supporting user-centred experiences.

</details>


### [57] [Context-Aware Visualization for Explainable AI Recommendations in Social Media: A Vision for User-Aligned Explanations](https://arxiv.org/abs/2508.00674)
*Banan Alkhateeb,Ellis Solaiman*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Social media platforms today strive to improve user experience through AI
recommendations, yet the value of such recommendations vanishes as users do not
understand the reasons behind them. This issue arises because explainability in
social media is general and lacks alignment with user-specific needs. In this
vision paper, we outline a user-segmented and context-aware explanation layer
by proposing a visual explanation system with diverse explanation methods. The
proposed system is framed by the variety of user needs and contexts, showing
explanations in different visualized forms, including a technically detailed
version for AI experts and a simplified one for lay users. Our framework is the
first to jointly adapt explanation style (visual vs. numeric) and granularity
(expert vs. lay) inside a single pipeline. A public pilot with 30 X users will
validate its impact on decision-making and trust.

</details>


### [58] [Unraveling Hidden Representations: A Multi-Modal Layer Analysis for Better Synthetic Content Forensics](https://arxiv.org/abs/2508.00784)
*Tom Or,Omri Azencot*

Main category: cs.AI

TL;DR: 本文提出了一种基于大型预训练多模态模型的通用生成内容检测方法，通过训练线性分类器实现高效准确的虚假内容识别。


<details>
  <summary>Details</summary>
Motivation: 生成模型的滥用导致虚假信息和深度伪造的传播，因此迫切需要一个强大且稳定的通用检测器来应对这一问题。

Method: 基于大型预训练多模态模型的潜在代码训练线性分类器，以区分真实和生成内容。

Result: 所提出的检测方法在音频和图像领域均达到或超过了现有强基线方法的性能。

Conclusion: 利用大型预训练多模态模型可以构建一个通用的生成内容检测器，该检测器在多个领域表现出色，并且计算效率高，训练速度快，适用于少样本场景。

Abstract: Generative models achieve remarkable results in multiple data domains,
including images and texts, among other examples. Unfortunately, malicious
users exploit synthetic media for spreading misinformation and disseminating
deepfakes. Consequently, the need for robust and stable fake detectors is
pressing, especially when new generative models appear everyday. While the
majority of existing work train classifiers that discriminate between real and
fake information, such tools typically generalize only within the same family
of generators and data modalities, yielding poor results on other generative
classes and data domains. Towards a universal classifier, we propose the use of
large pre-trained multi-modal models for the detection of generative content.
Effectively, we show that the latent code of these models naturally captures
information discriminating real from fake. Building on this observation, we
demonstrate that linear classifiers trained on these features can achieve
state-of-the-art results across various modalities, while remaining
computationally efficient, fast to train, and effective even in few-shot
settings. Our work primarily focuses on fake detection in audio and images,
achieving performance that surpasses or matches that of strong baseline
methods.

</details>
